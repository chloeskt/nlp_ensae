{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CANINE \\& Sentiment Analysis\n",
        "\n",
        "This colab notebook is meant to be a showcase on how to run the code related to Sentiment Analysis/Classification task developed in [this Github repository](https://github.com/chloeskt/nlp_ensae/tree/main). \n",
        "\n",
        "This project has been done by Chloé SEKKAT (ENSAE \\& ENS Paris-Saclay) and Jocelyn BEAUMANOIR (ENSAE \\& ESSEC).  "
      ],
      "metadata": {
        "id": "6FHrpFHhrOZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Description of experiments done\n",
        "\n",
        "In this section, we are interested in the capacities of CANINE versus BERT-like models such as BERT, mBERT and XLM-RoBERTa \n",
        "on Sentiment Classification tasks. CANINE is a pre-trained tokenization-free and vocabulary-free encoder, that operates directly \n",
        "on character sequences without explicit tokenization. It seeks to generalize beyond the orthographic forms encountered \n",
        "during pre-training.\n",
        "\n",
        "We evaluate its capacities on sentence classification with binary labels (positive/negative) on SST2 dataset. We have \n",
        "whosen this dataset because it is part of the GLUE benchmark and as such is a standard way of evaluating models for \n",
        "sentiment classification tasks. We monitor the accuracy obtained by our CANINE model and compare it to BERT, DistilBERT,\n",
        "mBERT, RoBERTa and XLM-RoBERTa. Note that only mBERT, XLM-RoBERTa and CANINE are pretrained on multilingual data. mBERT and \n",
        "CANINE are pretrained on the same data while XLM-RoBERTa was pretrained on 2.5TB of filtered CommonCrawl data containing \n",
        "100 languages.\n",
        "\n",
        "A second experiment is to test the abilities of CANINE to handle noisy inputs such as keyboard errors, misspellings, \n",
        "grammar error etc, which are very likely to happen in real life settings. \n",
        "\n",
        "Our third experiment consists in confronting CANINE to a more complex and noisy dataset: Sentiment140. It is made of 1.6\n",
        "million of tweets hence the language used is more informal, prone to abbreviations and colloquialisms. From reading the \n",
        "CANINE paper, CANINE is expected to do better than regular token-based models which are limited by out-of-vocabulary\n",
        "words. \n",
        "\n",
        "Following the previous experiment, we decided to test how CANINE would perform on Sentiment140 without having been train\n",
        "on it. It would allow us to see how CANINE and other models perform when faced with \"natural\" noise (language in tweets)\n",
        "and when the domain is different (in the sense that the topic and the way of writing/speaking are different). Additionally,\n",
        "we can quantify the gain in accuracy from directly training on Sentiment140 compared to doing zero-shot transfer.\n",
        "\n",
        "As CANINE has been pre-trained on multilingual data, it could be worth it to analyze its abilities on other languages\n",
        "than English, especially since it is tokenization-free and hence, theoretically, should be able to adapt more easily to\n",
        "languages with richer morphology. To test that, we did zero-shot transfer learning on multilingual data (MARC dataset).\n",
        "\n",
        "To go further, we decided to compare the abilities of CANINE and other BERT-like models when actually finetuned on this\n",
        "multilingual data. To do so, we have chosen to work again with the MARC dataset, using data in German, Japanese and\n",
        "Chinese. We would like to see how CANINE compares and if it is better on languages which are more challenging for\n",
        "token-based models (Chinese for instance). Compared to the previous experience, we are not doing transfer learning but\n",
        "really finetuning for 2 epochs the models on a train set.\n",
        "\n",
        "Finally, we provide a look into the prediction errors of all models on the SST2 test set."
      ],
      "metadata": {
        "id": "QqRP8NT-ExWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "zsvd33dRt4KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository containing all the code\n",
        "\n",
        "!rm -rf nlp_ensae\n",
        "!git clone https://github.com/chloeskt/nlp_ensae.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BluGJsjtzLw",
        "outputId": "b413814a-1034-4043-eb32-89025980f1c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlp_ensae'...\n",
            "remote: Enumerating objects: 341, done.\u001b[K\n",
            "remote: Counting objects: 100% (341/341), done.\u001b[K\n",
            "remote: Compressing objects: 100% (240/240), done.\u001b[K\n",
            "remote: Total 341 (delta 188), reused 242 (delta 93), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (341/341), 351.77 KiB | 10.35 MiB/s, done.\n",
            "Resolving deltas: 100% (188/188), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WHjRKrSnBJH",
        "outputId": "f4543145-09dc-46a1-ecbc-4cbec8d67834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 22 13:09:22 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    25W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install some dependences\n",
        "! pip install --quiet pandas datasets transformers nlpaug"
      ],
      "metadata": {
        "id": "2BXBI3aRt6p_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853d4e10-9f65-4444-d857-ce35d646a7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 325 kB 14.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 90.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 410 kB 67.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 85.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 83.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 99.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 90.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 84.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 83.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 86.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 90.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 97.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.9 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "transformers.logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "oAd1GmVSB8qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your google drive to save results\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NQiBGT8Xt8iH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64bac686-c09b-4c9b-e32e-0b1eb8b1b317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pretrained models and custom datasets\n",
        "# please note that sometimes the download fails for unknown reasons\n",
        "# But all files are accessible with the link https://drive.google.com/drive/folders/1HjKQ_C_EoBDncjA3nJ-IlgjwhQe4bKTO?usp=sharing\n",
        "\n",
        "!gdown --folder https://drive.google.com/drive/folders/1HjKQ_C_EoBDncjA3nJ-IlgjwhQe4bKTO?usp=sharing -O /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "H-l_I3AKhhar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe3ce51-1ce2-4a64-d4b3-a0a5feacf2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder list\n",
            "Retrieving folder 1-LLJWl5Zm3TP8-_J_S2S0Ak3f6GoioVU bert-finetuned\n",
            "Processing file 14lNatLs-jOUo8wPuYb3-4CXC2V-9URE2 bert_best_model.pt\n",
            "Retrieving folder 1C-b64k0gxGDIK1aSHzvtiVdjP_IUcjBs canine-c-finetuned\n",
            "Processing file 1HKShAnDd2iCMYDoCrTXW-ppfmZg_VKk1 canine-c_best_model.pt\n",
            "Retrieving folder 10GSpIxLSFTXHjd86gS0hgYqOWk3BW87m canine-s-finetuned\n",
            "Processing file 1485SbxkMNdcX8qV42_7qX0TPkfFU0JZ7 canine-s_best_model.pt\n",
            "Retrieving folder 1Qd2jzOGMa8ViDjAOU-xQp3aMNyWv9HcC distilbert-finetuned\n",
            "Processing file 1g-Dz_5GnXoC-rpsntmSunYnMcIGKRTfD distilbert_best_model.pt\n",
            "Retrieving folder 13SbRlaGyJSdSGqf_esB_QrKHaoYfzvJs mbert-finetuned\n",
            "Processing file 11AWI2N9QGhpSjRSLdRh7fyf7B0Hc8ifA mbert_best_model.pt\n",
            "Retrieving folder 1HLMfj2ok75zyj7tJeRv6Y3BmXFHXooJ3 roberta-finetuned\n",
            "Processing file 1YkKuD1wW4vVHeiaLy8I-6l0ropHRAS-x roberta_best_model.pt\n",
            "Retrieving folder 1LK7y9ekKlQ92sxDLlkEfh6agBw3dtX92 xlm_roberta-finetuned\n",
            "Processing file 1XBGWrvbzlfE0-mAELwxJVDIKAs5n5Ars xlm_roberta_best_model.pt\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14lNatLs-jOUo8wPuYb3-4CXC2V-9URE2\n",
            "To: /content/drive/MyDrive/sentiment_analysis/bert-finetuned/bert_best_model.pt\n",
            "100% 438M/438M [00:02<00:00, 179MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HKShAnDd2iCMYDoCrTXW-ppfmZg_VKk1\n",
            "To: /content/drive/MyDrive/sentiment_analysis/canine-c-finetuned/canine-c_best_model.pt\n",
            "100% 529M/529M [00:02<00:00, 228MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1485SbxkMNdcX8qV42_7qX0TPkfFU0JZ7\n",
            "To: /content/drive/MyDrive/sentiment_analysis/canine-s-finetuned/canine-s_best_model.pt\n",
            "100% 529M/529M [00:03<00:00, 152MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1g-Dz_5GnXoC-rpsntmSunYnMcIGKRTfD\n",
            "To: /content/drive/MyDrive/sentiment_analysis/distilbert-finetuned/distilbert_best_model.pt\n",
            "100% 268M/268M [00:05<00:00, 48.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11AWI2N9QGhpSjRSLdRh7fyf7B0Hc8ifA\n",
            "To: /content/drive/MyDrive/sentiment_analysis/mbert-finetuned/mbert_best_model.pt\n",
            "100% 712M/712M [00:06<00:00, 106MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YkKuD1wW4vVHeiaLy8I-6l0ropHRAS-x\n",
            "To: /content/drive/MyDrive/sentiment_analysis/roberta-finetuned/roberta_best_model.pt\n",
            "100% 499M/499M [00:03<00:00, 156MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XBGWrvbzlfE0-mAELwxJVDIKAs5n5Ars\n",
            "To: /content/drive/MyDrive/sentiment_analysis/xlm_roberta-finetuned/xlm_roberta_best_model.pt\n",
            "100% 1.11G/1.11G [00:09<00:00, 121MB/s] \n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pretrained models and custom datasets\n",
        "# please note that sometimes the download fails for unknown reasons\n",
        "# But all files are accessible with the link https://drive.google.com/drive/folders/1A9SrIW0hyXSyj0joKBC9re-U4ri9zewt?usp=sharing\n",
        "\n",
        "!gdown --folder https://drive.google.com/drive/folders/1A9SrIW0hyXSyj0joKBC9re-U4ri9zewt?usp=sharing -O /content/drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxPtmN0AX-CV",
        "outputId": "29929d25-cf5d-4434-f32c-8753a70f1367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder list\n",
            "Retrieving folder 1o9TBDIWpMY4xZdw6o8aeEjPHxQwDBXdj amazon_multilingual\n",
            "Retrieving folder 1FuMgJ6zIPtHrzMkM3EbD7QqmGgfQERje de\n",
            "Retrieving folder 1-8wnPWniOLNSY7_ExYM4mVWmRgqGH_j9 canine-s-finetuned\n",
            "Processing file 14ptaG9y6X6uwYf4GqRZkiT5XxyV4uoId canine-s_best_model.pt\n",
            "Retrieving folder 1vNfagnU5OgcsUiTOyqqHg2mkQE9J_21i test\n",
            "Processing file 1mDbXDvyW1YbeiA0NROqwRdgOOF6UfEp9 cache-0cf405bf6d7327dc.arrow\n",
            "Processing file 1LLSS0fRy6u4TRY-Fmxl-rUIHZIBF6Lm2 cache-0ff9c1dea035819f.arrow\n",
            "Processing file 1_PkU68qOcd9vxyrSQWVPPbu1VVTZbgjI cache-30e7cd049319cfd2.arrow\n",
            "Processing file 1qVp6kMeoZsrbgntO5vBYPIhGwqXBTvMh cache-799e77ac4d257bac.arrow\n",
            "Processing file 139dxZmgiWy0LkHm7422Myw4aMQkrd2N7 cache-ab14ae5ad6b924d0.arrow\n",
            "Processing file 1-6kVANED6LPTnLwWTbS5npvES36C4z-7 cache-ffca98777b678b04.arrow\n",
            "Processing file 1D4teKKM1bF4imSVnIg3Cb7fkwEozgdCy dataset_info.json\n",
            "Processing file 1Mu-GetN15oWscZosSXNbR20NcDBbSS9E dataset.arrow\n",
            "Processing file 1AzJd6PboyfyOAIO68cqtGzXBgiI1s2Wh state.json\n",
            "Retrieving folder 17argPnV4ypjriMXaZYTyX-eLHNjMKmCR train\n",
            "Processing file 1PPBNEj6ftdofsYuQ59M_J85yP1PSs2mS cache-3e52fa294a49ee1a.arrow\n",
            "Processing file 1KasXe8jYFDRE6iIonx_qOUlH9Xp7BgCx cache-8ddeb733db03c67b.arrow\n",
            "Processing file 18rmCVT7nnqBUCoZkVoLefWfglRzB7AQN cache-43de474c5b12fe18.arrow\n",
            "Processing file 1guryQJn2CRoSoKOgYRLXPKqyt7QRgIJT cache-476d443084b39d41.arrow\n",
            "Processing file 1Ft-moew0TTUQsC_AXAaydi6f9A2uwc0S cache-e82e309ff8c119ec.arrow\n",
            "Processing file 1pm27-dvbMY7Hr9WVz6Woc4ns_wDwQXOC dataset_info.json\n",
            "Processing file 1jfV-jmh-YwFRVfanzT8-5wnviekttpYN dataset.arrow\n",
            "Processing file 1YpJQUf6rAclygsnPzSG4nci9qnyF1FfO state.json\n",
            "Retrieving folder 1YEjMXYI_4VG0M1qwnJoIT07xE2wQx2kM validation\n",
            "Processing file 17ujzxHwtHF1EV6rgGgi4GZUAx0RXND73 cache-0bd30a7acead8201.arrow\n",
            "Processing file 1DRWb-JLl-Rn37Z_g-ILNeXsVXqI-CA9g cache-60ffc76d8ce840c4.arrow\n",
            "Processing file 1srDj3TKAb1Df5HTJhjjTtcBHZc3IyNu1 cache-a07f0cf37eb866af.arrow\n",
            "Processing file 1-4U2YhAw3HZ_2Dmyz3Hvit2MpvAg3jtX cache-abed20c343b2472e.arrow\n",
            "Processing file 1pNPgZ3FcPspg0_R3zi3OTRczsCVza46t cache-d399bd1ee9cc637a.arrow\n",
            "Processing file 1UGpwxYWeBDJhXdbDlT3NCcoSybmkdeaS cache-ebe72e447ba597f7.arrow\n",
            "Processing file 1pb5zqzLhZk3Vp-w-5IRuk0EnLae9ddus dataset_info.json\n",
            "Processing file 1yOqVRFtz1bTxQwyNekuiiexNUI6B3Fa8 dataset.arrow\n",
            "Processing file 1TZX7jZ76VTRSGelJK_eK01amkr8-zQjx state.json\n",
            "Processing file 1wPJU2NcyP6oBYIVoFdtIyxXpyaihUN8p dataset_dict.json\n",
            "Retrieving folder 1hCWD_sb0b0UK7Olf610gRmcUx_n3VTuQ ja\n",
            "Retrieving folder 1-7-uqGshP8yVMBevH5YcBCenmhU4avyj canine-s-finetuned\n",
            "Processing file 15Rrow5CHdiOymMf0Qm2ZCz8Ef4umPAoZ canine-s_best_model.pt\n",
            "Retrieving folder 1HioAWJzOgei0SsVXitB7XjvkxvoIL1T7 test\n",
            "Processing file 1XPs-9N_icKQWQuPm7Ud2HFj00B1PWzxI cache-0c1bc5ae63957842.arrow\n",
            "Processing file 1S0vV9TLb_XQWrjAq4R2X0SjJh2m1LDve cache-5d088d4a78bd2f3d.arrow\n",
            "Processing file 1QG-9SV3ESKGzsQzsh4v4Iy7wp0V3zpYI cache-08f7182ed3e203da.arrow\n",
            "Processing file 1-5n2EpQuNOj8eNnIfmOkuS_bqmMmceiz cache-520348d4bba79eb5.arrow\n",
            "Processing file 1nPB6Fv6H-l32XkVZEijaGMin1qW8Kc-3 cache-d8ac49770ef39b52.arrow\n",
            "Processing file 1-s8RnGXAOpJ8agg8Tfz67v6SHq2pvE6K dataset_info.json\n",
            "Processing file 1XGfwkzOLPB1OpN8pFVEV0GnXJrzhxbgO dataset.arrow\n",
            "Processing file 1KSuLRMlecO36tnaYKAv6hY9VB9YejT89 state.json\n",
            "Retrieving folder 1vTy7w_s-L7xKDWPyTh2z6q01XqyxZnr7 train\n",
            "Processing file 19ncboSKpvraR8N-PX4QiI_q_Xt1xr1Ft cache-1bb346c7b7e2ce1a.arrow\n",
            "Processing file 1xvmV58-mXDRqti8vK1jpNBakl-opbtr6 cache-59f3a6d32b5cde14.arrow\n",
            "Processing file 1-xkKFnBt4am0cGEqk-UIwuzNahbFEPcs cache-89ef374b28d5fdf0.arrow\n",
            "Processing file 1Q15dPTyp3l4jZNlZk10Ir6XCxYgsIgD- cache-9558603f29e07ef1.arrow\n",
            "Processing file 1TWSPNWf_AawLzsgdGsWAshOqantZEyCD cache-b2e198164b070924.arrow\n",
            "Processing file 1PolX0gPAmRcdWmYPH9v6_YA7eIlxy1ra dataset_info.json\n",
            "Processing file 1MMgRJLzxgY4ywgTOicp9Agr417hlOewl dataset.arrow\n",
            "Processing file 11bPQ1xtRQ56DR7cbaokbakiZBqRk5OfZ state.json\n",
            "Retrieving folder 11LsDx62nvYkun9vrEivyVz2_2UC4XesZ validation\n",
            "Processing file 1JbYAWQzyA46zs0gcsYzssp3W296vgxyv cache-0c9755fbb7a3693b.arrow\n",
            "Processing file 1tJR1ZCJ4WUx7FDrhDbRzVBD-SKq7LOiL cache-1c451ad79b31956f.arrow\n",
            "Processing file 1-2fL5fx5hQq1aVo-CJ4tnauPFmcJMraI cache-5de2efad995c4219.arrow\n",
            "Processing file 1PLHRWMBIt_m6YRfr9XZawtg-Ulq_-Avm cache-4697bce846e168a6.arrow\n",
            "Processing file 1x-TH0P7nm-lA3MiF85qAMmMp9nQ8LBa- cache-af3398eee0d05110.arrow\n",
            "Processing file 1BZ0C91DRzuYeG9Fx77tYVeF8rp0_IWXE dataset_info.json\n",
            "Processing file 1VF0f3IBaoum4TzDNKt8Lmn2-lYggpyF- dataset.arrow\n",
            "Processing file 1pOW1Hz7GBDDu_m35tfuDbVrafc2vP284 state.json\n",
            "Processing file 13y87fQYUsy-C-1KD56W64rLI2bI6tLL_ dataset_dict.json\n",
            "Retrieving folder 1Z0vfd_WRr1-mPpWHbTLgiHp6CfZuDy5b zh\n",
            "Retrieving folder 1-3YnTDg13Mzr3pLSM06hCEGotQAlYmnf canine-s-finetuned\n",
            "Processing file 17cgFweNsMxT_QzwHBfFg2Mk9q03VS4yy canine-s_best_model.pt\n",
            "Retrieving folder 1mq7ZUDWyjQacEHUcRrK4TK0yp5PDHT7h test\n",
            "Processing file 1bjf0YxlysvmAoBwqALCNT2rnQISr9GFF cache-23bb1df8586b9cf3.arrow\n",
            "Processing file 107oLablx8pGa6DEUpcjwMxg0_C4wYR5A cache-85f2b3b780db7cd5.arrow\n",
            "Processing file 1Cglmnki2aK1LVXOjwPpFYSwI4kdLYtCl cache-446b64f96bc29686.arrow\n",
            "Processing file 1-36W73bYuFrf1Wot211tKUWvz9cq_VCO cache-be5eacc001271ec7.arrow\n",
            "Processing file 1-dbfYmsPiJM3GYqOSIKFrQEkiGT-Bj0- cache-d1fcf53869baa8d1.arrow\n",
            "Processing file 1UUcCud2PVNOuBX_Jb2mOKm39fcSOHrCq cache-d6880f92cdd55a5b.arrow\n",
            "Processing file 11UN2z0WpoUqK9OlRJIr8PA8qsyr2vP-u dataset_info.json\n",
            "Processing file 1dFZ4y7uCKI3qNLNwrgVVTRUTtSJ8luLv dataset.arrow\n",
            "Processing file 1RKC4ZIICabe8yvuENt2_56STHX0EeljF state.json\n",
            "Retrieving folder 131bFcSnU7_IbN2J1pYEYbpUBT3HgMWvQ train\n",
            "Processing file 1Yg23rtvc-iEo5sucZrpNH_4RcvG4bwlh cache-4f3aaf339d379547.arrow\n",
            "Processing file 1kTNuJDUMcDbIVlKedAfmdEHe1uMgJnL2 cache-52da0459d60f6c6f.arrow\n",
            "Processing file 15eRvljVvtgMe3nmensO1i8yjZPezQDg_ cache-81b23d790ba69dd2.arrow\n",
            "Processing file 1NI77s2QJDH6JWDEr9K9DB6TXRWvj9NWC cache-aa62bf46d5343764.arrow\n",
            "Processing file 105ENVL8q711PL24U5rT0vfvWxmb2QXwJ cache-b5fbf11e2711b4e0.arrow\n",
            "Processing file 1f-MO2-vnNDwH6gt6dqy134cpFxfI-32w cache-c358bfdc13ba5f84.arrow\n",
            "Processing file 1BjvjcKmbp9Aq95uokB38PU9ALJUJ5jC0 dataset_info.json\n",
            "Processing file 114KVKblBwXocmVFiiM4oJsEGEhgjxWep dataset.arrow\n",
            "Processing file 1ifkJrQ64G76FwvuoZOjGsJMFu1EeM9Bl state.json\n",
            "Retrieving folder 1O_FzzhM0s3ZpLlwLFoNWoHmEW9P0a877 validation\n",
            "Processing file 1iFqUtj0LfryrbuIW9mQxD9GPWBbmmGrv cache-8fb594abe1901fef.arrow\n",
            "Processing file 1--akmX5i9U8vNMaHbZ-uaCOFNzOG4TQL cache-63a1724557510dbd.arrow\n",
            "Processing file 1tbxoiMXsEkRnnYHuAPDzm7s8_qNMf_ht cache-64a613ac1fe646be.arrow\n",
            "Processing file 105ccVHU9mpc-tKkRAJuYaz3HQMQxw9pF cache-80f73a88f123deed.arrow\n",
            "Processing file 16X-LdiUJs-I8P0GJDOTQ4y2QBlbTJVRs cache-c8e8bc647a1ad8cd.arrow\n",
            "Processing file 1oh9b5FrzZvu1mrR1KQdezdZKbo84eLPT cache-dc7620183157ed52.arrow\n",
            "Processing file 1V-62eZnh7FJe_ssJZt58lUy4uYY8EYO2 dataset_info.json\n",
            "Processing file 1uKiw6D_OwpfMGB4CPrh4zoGNbjlEL4Ny dataset.arrow\n",
            "Processing file 1YJBgV7916TPr2Si7we6zAUrJFbyBNNoy state.json\n",
            "Processing file 1Eivrn2FqSw1RLmJ3M3z6KMVrglJjmRd9 dataset_dict.json\n",
            "Retrieving folder 1M0_OF5IgragrKh_mpvfBHPOffT0H3Eeb noisy_data\n",
            "Retrieving folder 11hn6FHKpFt4pHVx_tC7OvGRcEGweZDyX noisy_data_10\n",
            "Retrieving folder 1XkR1k7v8PcyICLc6xSiSmJMIEd_lOF1I test\n",
            "Processing file 1Ja4EKx-m-tUXYkmcbi7XVYwSglIkw_Iv cache-0fc11c810de328a2.arrow\n",
            "Processing file 1mb_KxE6iuGRT5ZQh-LkSUx4QAKsGe-TL cache-1d924c78eca9e55b.arrow\n",
            "Processing file 1bx2Fmuc54fcgkwokYrDyQr8DEQd7a3a_ cache-2d29b75010fc42df.arrow\n",
            "Processing file 1cW2FstOBN_p7QL-7Tu2uhtKJsm5Fo8Qu cache-7fe34a400c7ba6d9.arrow\n",
            "Processing file 1LdhwPGuR0Nd7g5-UG7AZZgAl_5KwwN1c cache-09efd87cf5ca8b88.arrow\n",
            "Processing file 1OTe8I2EU36yP_gO_WSkgO6JJaD_13BOP cache-22c4e1a83265c003.arrow\n",
            "Processing file 1VRsN2dEGV8OTaUdjXXq2QXMOchbbjgWT cache-576f4a908037b0ec.arrow\n",
            "Processing file 1WgNWKmN0zaYelJVlyUl4xosSQGY8SBdN cache-658d0a7bbf52c882.arrow\n",
            "Processing file 1RPe6O1Iz0jSxGXflt971L1BtpPkqHAEu cache-924fcaf2113ef155.arrow\n",
            "Processing file 1QDztcIhcXjMJtIIOzMhgMdiKPwppyZuK cache-0183900f1ffb88b8.arrow\n",
            "Processing file 16BJZWpnL96lBg8B4qeZqPnzRolTGYlRc cache-b6037eb4160ab1c8.arrow\n",
            "Processing file 1D67yYZ6IpWWa1SsmYIFPJ00uJ4Gan740 dataset_info.json\n",
            "Processing file 1zZHOyPMzSb5PBzD8Q4JxvNkLhAOpaZCD dataset.arrow\n",
            "Processing file 1EIG3QdU9yYYXV4A5yqS_RhLXevJi7C9f state.json\n",
            "Retrieving folder 1wAilWLwjIxNYN-bX9W5e6YZE4vGyY3se train\n",
            "Processing file 1uZ08LruaCwNs1xD3MSpUq2wVpsGQJYct cache-4addac44b485299b.arrow\n",
            "Processing file 1Ad45hOUPGlDWYdnKaMBmkOqgGmtLNGAq cache-04b8bdef36bc9fd2.arrow\n",
            "Processing file 1jqYSiqbdT3b7HCOTFu2lV4FpK4pyFmh8 cache-021c7814c1921ea7.arrow\n",
            "Processing file 1xM50_RUYduXSeTqkHejV1GPIBkjlZSil cache-46ab7fb1d6a82080.arrow\n",
            "Processing file 1v-brOE5rcEW29V3y7eAMh-oxeiuYIBWf cache-21714cf40b061451.arrow\n",
            "Processing file 1zFfNZG4EpW3jSYLmQtymnbcu31pEDrqs cache-63022cdd0f4e7e2d.arrow\n",
            "Processing file 1KXvRQDK8e3FqBG-YEsE_zdBWcWZthOgd cache-104491b5290c0450.arrow\n",
            "Processing file 1iDekEfSQ8YcToYGLeeYvkHEOLWZaVu9b cache-6600709899155179.arrow\n",
            "Processing file 1Mpfq6V4WrpIbMRYFpoosEdhSYayn3Ecv cache-a0cbb1fdfd12bba7.arrow\n",
            "Processing file 1eppM3zm47_ADh8ViP1DctpUiAarc14ug cache-b7116bb01f793aea.arrow\n",
            "Processing file 1Jr3164tJBV8_kg4vyo_jbPPoG2NRyf5l cache-e05f3dd84e66df52.arrow\n",
            "Processing file 1UYgPYU3V2gGPkeKNXpkY9QinsL8gUwlR dataset_info.json\n",
            "Processing file 14Wjo4syKhwx23VLLAsBJJA0acS9gzfg2 dataset.arrow\n",
            "Processing file 1O58nBobsCCTJCdmWpyVnnsNoM0DnkaQN state.json\n",
            "Retrieving folder 1nv1mZJSTW61KizZ1Lb5mkTkXOmQpBWjV validation\n",
            "Processing file 1v7hy_Dld-CJ3BXZkAIkS-GRozcJKv25C cache-50b69f42fac2f078.arrow\n",
            "Processing file 1eP7zr3vDbfBIxiJC_d1k7lwCJuQRNf8U cache-60feb7d8d3b971c4.arrow\n",
            "Processing file 1tGT9S7TiNHwAHYA9WySQiXXT7Qgyuj26 cache-74c2761a07e1c2f2.arrow\n",
            "Processing file 1fLYVn7nsnmeQbQZqIuKuNJV3dWvDzSdD cache-94f9cc188564bc31.arrow\n",
            "Processing file 1CjVRyfLXhPOCd86IOQIO9trQNO8tceak cache-383fb9c33cb65adc.arrow\n",
            "Processing file 1FZVIuByoMxhx75owxjVJfKjlG9Kv8-YE cache-433ab258ddd37144.arrow\n",
            "Processing file 1Gjx0_RLo_yv_cO6lP0TKJSVr-DliRKGH cache-910cff01dd330da4.arrow\n",
            "Processing file 1-MBv1_xiV40jVeKmB2B8JlIbCv2w08yQ cache-a14f1f17dfb6ab77.arrow\n",
            "Processing file 1xDTr78wSrTI2ZZRnh34SWveBEgZjZm0F cache-be7443b0391ae971.arrow\n",
            "Processing file 1a9p-M1fuNQZd7S70aF86ORhL_GTYPSI5 cache-c770f7e146cc50da.arrow\n",
            "Processing file 1gUbHkSoyd3upWAQ5wApDSSkg4Ur1P7n_ cache-d2622b480aca3ac2.arrow\n",
            "Processing file 1wcjGCl_eSnJ0F0LPFbTy1CdB4hQW7nj1 cache-fedf2a79663e0fee.arrow\n",
            "Processing file 13WjCV9EBeDp55xSbebzmyzaXsC0h8RJf dataset_info.json\n",
            "Processing file 1A3ktrhzkKo54GgbHl_uDJGg4OTj7AZkH dataset.arrow\n",
            "Processing file 1mzLLe1xwI_JfZUwkFONJ6QB4p_n3bjmB state.json\n",
            "Processing file 1ykByg4tNfzfFV8iwOnVrb6_nJhEtKbNk dataset_dict.json\n",
            "Retrieving folder 1bszvJRC74_DHC0m1UohdQdUjn0_pmk_X noisy_data_20\n",
            "Retrieving folder 1lg2pLuSJevw1JixW1is7Wy7dIkBS8ZHu test\n",
            "Processing file 1w3TFuvL-lYTugATfc54NcLGDbuplx3bs cache-4c8346ceaed714eb.arrow\n",
            "Processing file 13VzMAT9oNC6G1yDpNFMdftEj1u8Nwy3r cache-8e382c0a683ed6cf.arrow\n",
            "Processing file 1UZLw40rM4wWCn7m_H_VrNfNaP87YL-rs cache-8eaa7c1dbeac36dc.arrow\n",
            "Processing file 181g_q0Xxs1Xx_PWGztlKEeYGIFRiUBo2 cache-28ae93f335a53cdd.arrow\n",
            "Processing file 1Gz5xB_RIvWmHhDuNanN-1fyCmyuxYW6S cache-954b1d4934d5c706.arrow\n",
            "Processing file 1QMNQ08d2kgOsLiEmDKZldfaD1QcA3jQ1 cache-9192f57e8d7ec685.arrow\n",
            "Processing file 1U7KZlu0lydxp6RNYgibjIhx0Z9QhTB-5 cache-90548f9ab32d537a.arrow\n",
            "Processing file 1uIG-Sn77seugNqo7Q6QvxkSCLoYDpUzK cache-29436305a430629b.arrow\n",
            "Processing file 1TjODO_dBsz5sH8EfkJEbtFKQgxKTCkO5 cache-b04dc44a22128b36.arrow\n",
            "Processing file 1YN5C6gl6Eh3K02TBTR-1WSztZ3N1Xnzs cache-b8b028336b649cb0.arrow\n",
            "Processing file 1J_kH3di9BdzFXT-SlaDr93LFr7jYT9ec cache-e5f9e93a021cd607.arrow\n",
            "Processing file 1gY0iWjWAsolubAmjb3UI80EnhAJEtc1O dataset_info.json\n",
            "Processing file 1C4XDp8pRHdd9TndjjiuHDXgOCGjbi34i dataset.arrow\n",
            "Processing file 1esvgekklZdnOROXefc-3Y6hRY-vv2mpS state.json\n",
            "Retrieving folder 1XP-L_nvMcd0PiSNMwzp-7idGoQz4ean- train\n",
            "Processing file 1RfZZNTF-U5rH0AKExH_L2IBPFGkpvRCd cache-1c7a6377437e2de7.arrow\n",
            "Processing file 1WJoLgSuETD8mY27BHmaTpLTcgWFaitAL cache-1d072bef61c52943.arrow\n",
            "Processing file 1zHGWiMUj3Wq_SOHWQVJ7PHOCJXvG8t0B cache-1fb4c038f8556026.arrow\n",
            "Processing file 1dXgXXmrHWL1PcYjoVLQsdk36CWUshA2C cache-5a20783a1890d29f.arrow\n",
            "Processing file 1a51Jj3Wg84zAS7yqLiMqieh5nmZUGeXt cache-8e06b829d4bef20e.arrow\n",
            "Processing file 1SNHHxTfpmUnSlryEAOd--nkc45M4jfeo cache-5987becbbe40d7d6.arrow\n",
            "Processing file 10Hgj0io-UEkPefF-4rSvqtmX0Lx0Cm8Z cache-73314787c8b61851.arrow\n",
            "Processing file 1PX4H8ox_vV2rnS7LRB7d6Lukwx-B9pxQ cache-a473805f015711e2.arrow\n",
            "Processing file 1Z15478A1drtUiHk1GOdGkTRkLgTccDxZ cache-e8fcbc43bfe07114.arrow\n",
            "Processing file 1dRdg3PTtStHooJQn5efi_L5tjsQ_Zv4Y cache-e9ed9dab44986f89.arrow\n",
            "Processing file 16Vq7bEYEdtCMw81DJ92-joR9dIQxP-tl cache-f5ba547400a99b73.arrow\n",
            "Processing file 1T_ue0BGmGbPmChfOeVTXmR1Tz_8LoDIJ dataset_info.json\n",
            "Processing file 1xdw6xudBrAWHNbRqZjO0hsQui5DLszhK dataset.arrow\n",
            "Processing file 1AKsLFM82g3CjqAvV-no_YKho74cZDHrz state.json\n",
            "Retrieving folder 1tdoi9ViFWZBgXdhQdjx11bZh4RNxRynW validation\n",
            "Processing file 16GtrETZuEqSxTXlHXFCrjYlRPSdx5WhC cache-3a28d62281ad372a.arrow\n",
            "Processing file 1Yvv9IwaCrYUS2zyZDssFpJuyk8Y0V9PM cache-85a9ef04b18b1f63.arrow\n",
            "Processing file 18BWPV5VqCYEKjda_4PdXj1LJCYmnzDaO cache-883653e814fc0fd2.arrow\n",
            "Processing file 1trXD50dDQRqDw3D0ltPiVTuBb1U-Sa6s cache-a999ea49931acf21.arrow\n",
            "Processing file 1iT9a_h5Y0q_x01oIDJTaBeyQB4Wr6fKK cache-b9dbd8f7c9e5ffbd.arrow\n",
            "Processing file 15xrINkYNKTwf8icRHLJw4ahQ3R0YbnsT cache-b53fbfd6c058d037.arrow\n",
            "Processing file 1L1cuFAClRT2NAyIJKsSSGQOucqGF-Vc4 cache-c5f906707b46dfb0.arrow\n",
            "Processing file 1V-x2hyDGObZ5dEnuv7ivBjPIeMCxmaV- cache-d540b91bedb37094.arrow\n",
            "Processing file 1V3NbpfNkO6SzqjlqX_6ExBL7DlMOC-L8 cache-df8f04ab7ebdacab.arrow\n",
            "Processing file 1c6csurcPBRyJxe9eCWm7ux2vx2-vveHB cache-e65ac41b0cdd4cb7.arrow\n",
            "Processing file 1honuD_ZN8qWWfV2QzKUD3F4NdpqJxO1X cache-e1206839d1f3b3e3.arrow\n",
            "Processing file 1tGuQpaCBPtb4Qw8sIn-aNEOpfpN7o51m dataset_info.json\n",
            "Processing file 1oPGZvmm5mfuZT1Q4MaOdz8TqRpd8NWDn dataset.arrow\n",
            "Processing file 1AYGIUSYtK57BlOiUtzBtSZrJyEepHeoK state.json\n",
            "Processing file 1OC4ceDsKg6bLplmhHKrVR7axLo6QjJfw dataset_dict.json\n",
            "Retrieving folder 18IV8pVVHoV8890WwNPCOw-mlzeca8s_Q noisy_data_40\n",
            "Retrieving folder 1fC_yVbxI4hKsEprjYhXY_IIbooF0SAJk test\n",
            "Processing file 1B7FD0hLzx5BruvCd9o9piWlaVmRmdV_O cache-01ff3ae4ae08bd65.arrow\n",
            "Processing file 1t8jcCZNu-FknVnvEk7dDdCwRyYubIrCw cache-2a1bbf4e098434ee.arrow\n",
            "Processing file 1jRoZ0lfmcBtLfqDH1I6MYdvpmaaM1xJX cache-7ed7e30a7864b028.arrow\n",
            "Processing file 18uywdzHcCkcVenk5lYmX_DBIUwr6AB8J cache-9acada45a8a8c0aa.arrow\n",
            "Processing file 1b_cHfHPeMGyONd2vTzSnwyUdC09akbfI cache-44f61734b4bda589.arrow\n",
            "Processing file 1w5BlTQmPsk1B0xlQOBvFv41CwtcqM5Zx cache-262bbed13b354ac4.arrow\n",
            "Processing file 1DseN8GFux6DC-xb1K8HqwbKTHTUTQJii cache-482b2e2600eae012.arrow\n",
            "Processing file 1rQisyvWWjI8UiXNqPcSRAUG0WHrgc4u2 cache-73176890ea6dc6a8.arrow\n",
            "Processing file 1gooUEAnl1xHrxZadopGw__Rr0kJudvie cache-acf66e3129b34df0.arrow\n",
            "Processing file 1-VDkYrl8zO_75n7EBwpXzQBcGERtsPrG cache-db2823cb3d0ad657.arrow\n",
            "Processing file 1hXepZtT5FJckteKpTpO5JklCnqne_Jym cache-e5d75fc6329cbd86.arrow\n",
            "Processing file 1jTlHFvktoKG_jYtk3HcaaBED-iRTR5LC dataset_info.json\n",
            "Processing file 1qerG0sHH_QFbI9OidscH9foGG9zfe6YB dataset.arrow\n",
            "Processing file 1Ccok-A4FTlHojpMh6W9zuHcZwrYmKhLt state.json\n",
            "Retrieving folder 1MZbfX2DQezbuOKT4OU9T5jOXeCLYCYO9 train\n",
            "Processing file 1-NW5e8cg1UcpC6ULC5IzCbvhiMIl5L-6 cache-2ed0862112d811d6.arrow\n",
            "Processing file 1bUvLjkq9kP0M7KShom85t_b7qsQ62qtP cache-2fe630e1c652cf06.arrow\n",
            "Processing file 1ZW6COmyP6Ow7-cr8ujc8ARA2nbYw7btf cache-4d10a33f7b08757a.arrow\n",
            "Processing file 1htjTQ4a5ppDZGzRlejx22A9yczo-1m8B cache-522c657b966bcb2d.arrow\n",
            "Processing file 1uWq6VMVccuOiDMamdpJKXQ2xl0QNi3kR cache-968c6875e38fec3f.arrow\n",
            "Processing file 1dyLIq_3qW1hb0my_xNQlXTgaDI75nxa- cache-a5c308d3470aab5d.arrow\n",
            "Processing file 1jYsUT4P1zm_sSXOKt2B5B22NpdFsORC7 cache-b5745b5355e7819e.arrow\n",
            "Processing file 1BRm-7MFkauAIdL4YlZeosnVmi-MIGb0s cache-bc57f43c690a9305.arrow\n",
            "Processing file 1J2Rs2Fh5TfJ8TnE4wxF1zxuvl3mrzLfw cache-d1a22f3bf7a6a69f.arrow\n",
            "Processing file 1gZFV7rx7o79G2v_rCpk5tqeabXbSlKlC cache-dd433c056ae089c8.arrow\n",
            "Processing file 1ygE3JGcVD8TG-Y3VT7FnIinjdrrk-oIz cache-e336d38940ac1e60.arrow\n",
            "Processing file 1E6R5xFZ-bcXfLr-6MY_lEPG__D6cVfuM dataset_info.json\n",
            "Processing file 1nK24R7uwqgTw9y6Bf3GWw2Be_kI2kwnG dataset.arrow\n",
            "Processing file 1nYSFrEcdYG1mb-cIf-1qKr_PorZVCVM8 state.json\n",
            "Retrieving folder 1PEbd4rAVPawHrBt4L7iPJTBymY6irKrq validation\n",
            "Processing file 1uCWIgX3V26Mx_1pTyVCu2OIbgLbiypln cache-1c3d262aa54909c4.arrow\n",
            "Processing file 1i90GiN36A1NvjHyU9_VDvr1vyS_-VGAu cache-4fe8ad0ec5bb20af.arrow\n",
            "Processing file 1sMEiED0MO8RFguTri-zrkkHFzIvEdRiY cache-5b72f875a1f61723.arrow\n",
            "Processing file 1-NhBV_ZXZxXLa7iFntCVqpAoWDB0uf7P cache-039947ca08aca6f5.arrow\n",
            "Processing file 1wBO-2stW2Ckr6S64ZZ61c5L2fdNK5R35 cache-47531e205c6be279.arrow\n",
            "Processing file 19mkIq2P6Ud0NqiTqA77V97Q5wV5FXKO4 cache-96741ef723a588d6.arrow\n",
            "Processing file 1zOSXfuiT179rWyuieMy9F6HoL-h8SIG0 cache-a925b186e21abdd0.arrow\n",
            "Processing file 1KosTtctuAJ-N955XfP28D4500sUxZaDa cache-ccd118342665e6e9.arrow\n",
            "Processing file 1PbJ2g5p2inBTZAh-171MDc1vYgrmHFyc cache-cf409132fa910008.arrow\n",
            "Processing file 1Hg3mcfNIWWKbt3QFptdU8TDuMo_tsT9G cache-df8e154fcb071c84.arrow\n",
            "Processing file 1QfNqqhZ1aZVXGbR0_JFiEtZnQRhAD___ cache-e52996039e745a48.arrow\n",
            "Processing file 142HgGPGuglXf0yQ6lVC3NSUpPxps0Xzb dataset_info.json\n",
            "Processing file 1lGPLE8RcMam9SEAcmeKqFcM_y9V8szWU dataset.arrow\n",
            "Processing file 1WHnqWI5zTbnoHoemdPa7etgbjiFrTKhp state.json\n",
            "Processing file 1GLafMt1NNBMMjgUgDxAfYPtyb0FbM31J dataset_dict.json\n",
            "Retrieving folder 1zcLTJEgx3LXcuNHUWflixSpJdnYfEZvu sentiment_analysis_140\n",
            "Retrieving folder 1-1KGzzqtuj-nAlBtGr74yvZAD7M3KlvX canine-c-finetuned\n",
            "Processing file 1-9pJa9rUsbJ49rE1pOxA_rIyKi_2zquC canine-c_best_model.pt\n",
            "Retrieving folder 1-Bcny8myDUQlsMwD2iQ5Ia5fAZy60r3D canine-s-finetuned\n",
            "Processing file 11wyU5jmJsDf84aeNyumDgGr769YSrpjZ canine-s_best_model.pt\n",
            "Retrieving folder 11yRaZGgjLDWD2gHVck16edycZi24DS_G distilbert-finetuned\n",
            "Processing file 13wpGbYNdlJscPq5r7f7OY1nyLnzr2CcP distilbert_best_model.pt\n",
            "Retrieving folder 1Mw2MVm5oJsuawDd0rmCqQFa1uF1NIYuy sentiment140\n",
            "Retrieving folder 1mjSpfN-KmJzFmV5bv3n5H8qY6Cn9IrWU test\n",
            "Processing file 11sxbBSWPXpckULxihy3NEZmLbdTOd-fI cache-5a526dc79669ca8f.arrow\n",
            "Processing file 1-AqyXJl4FYxrSHYcMVJpuCwb7ExdlJwT cache-8c69694945f3e414.arrow\n",
            "Processing file 1s-Mn5EBf3GaSbnhkEF9HVYHFM1c3XML7 cache-8283ada9ef1b4bc6.arrow\n",
            "Processing file 1hMLdCbi86dmVCnGfkkIuDwrqQPCglgnL cache-e5099ad3aba559b9.arrow\n",
            "Processing file 126TEwZpZvVaDWBnlRcd-76Oq8Ye3hkg2 cache-ed21d313160e56b3.arrow\n",
            "Processing file 1ebbVp3srxyZWpsb3m4ujE2p4nXdoEDm6 cache-f809c12842bec088.arrow\n",
            "Processing file 1Ga25jJPewl7cP-vHe_dciGgksYvhTCZo dataset_info.json\n",
            "Processing file 1UpHatEBBe83YyYwdIePNVrTz9n5uCcz7 dataset.arrow\n",
            "Processing file 1Qz9EFL4Ysl5xc0lOQ63KjBHY9RVWIm7Y state.json\n",
            "Retrieving folder 1fM7YI_GyVp-b3KcrhKomTE2f6kxwGyXe train\n",
            "Processing file 1l-0Wrtm2XODyj3BcvkedEHIAXPfxL1k1 cache-4d4b58a1c98efd47.arrow\n",
            "Processing file 11pdEOoSc2EXTA5eBJ6QRyKz81dS7e1yB cache-7f56d56154fea18f.arrow\n",
            "Processing file 1-9ar1gyTWaI0kzTbkPv7-IaxO6ZV_xXX cache-28d60ce512666fe1.arrow\n",
            "Processing file 1cPxDZi2afJkQXbz8H5pnKaKNy1IQJSyO cache-89a488c86bd0fc22.arrow\n",
            "Processing file 16i7JeTfrShWlLI7xFkexopSRs_LIT4x- cache-67976c176a102db9.arrow\n",
            "Processing file 1AovSr0kNYP_qhz3oMjiQnUxumrjJ9QUh dataset_info.json\n",
            "Processing file 1PLJ8L-6iazwP-T1BxJJM89Zs7UVYui5N dataset.arrow\n",
            "Processing file 1p93NtrPbuNPhSaEfsyOshhrJL_pYCunK state.json\n",
            "Retrieving folder 1jqBYmuAJIw2WpvnG7iZ8iqugBSSzsY07 validation\n",
            "Processing file 1--itGQajdsMhYnd_2_Uf8WVemU7CankX cache-1f2a845177443a93.arrow\n",
            "Processing file 1-0HZFfK7tSDMZCJUySHvqkGzxxYJLSzO cache-03d55a58fb87fe5f.arrow\n",
            "Processing file 11qounRUd5fYwniL23XTPrPFLQkMUszbg cache-781e7ddb2348974f.arrow\n",
            "Processing file 1-CvepGOBnb6yOLdcnAvFFhnOi90aG_Iy cache-b613e1aa4e6487c8.arrow\n",
            "Processing file 1-1K_RGunAnP1uxBe6quXllGmd6klOPZS cache-e24a7fa17c53b809.arrow\n",
            "Processing file 102DSLNS4ost0-8dzk_nMgZkiF-b_9Coy dataset_info.json\n",
            "Processing file 1-wA45UACQlQZIj0q_Vw8LZ3cL_rbU1lD dataset.arrow\n",
            "Processing file 1Fk8aIXqqshNywxvRhZ0EVm8Y4fDdVJP3 state.json\n",
            "Processing file 1QbRhCCBD4qP5PFhVmNhBwXBNDnUF0DWE dataset_dict.json\n",
            "Retrieving folder 1jgkEsP42sMkY8ttATl4OIMblzmIsps63 sst2\n",
            "Retrieving folder 1iGQDOu-ZfeWDB2I2-i4ep3aKfRqnecgB test\n",
            "Processing file 1WBs2Y_SFHjVaSCXI6WM68iG0nSiI_uaM cache-80ad70ce9a1f8bff.arrow\n",
            "Processing file 11M6-TS4OlEfn68_xFrXUveS0K7raRyUG cache-bb221a77a8e78207.arrow\n",
            "Processing file 16qUrUcvUliO6mWzLTXeN4s578XsnNnqW dataset_info.json\n",
            "Processing file 1KVG-tfURUuz0j5EbfsmL5eBjjUVLdoz5 dataset.arrow\n",
            "Processing file 1tXsZLVJHxvyNM6KVdn0ii5UCCvlaT1_5 state.json\n",
            "Retrieving folder 1tHe0i66onbq5KO0xT_pPfOgv0At5QzQP train\n",
            "Processing file 11M1KlR5eei_ePhz4K5WApLZvRgmKAjha cache-3084f00c2afee77a.arrow\n",
            "Processing file 1MTC57GTMO7CuYFbR8WCTV4aBUHgqsZEt cache-a39bfff7b2a6f2fc.arrow\n",
            "Processing file 13zSuQ6R3vdcTzJLiBepuLNCC4OsRX0q9 dataset_info.json\n",
            "Processing file 1cXav5LkZYHnjUBk8r_wfdp8sVg_4wbFk dataset.arrow\n",
            "Processing file 15s8i83ShAwCG2axkGUaTdJ_GlmnbZQEP state.json\n",
            "Retrieving folder 1N2-B9d2ChVDgaAsUPKgFYFRGwLlJHv6r validation\n",
            "Processing file 11M2Q3L_DBK8_bKlbUK4kxGKhlWyBGIOC cache-2e709a3490f5f9f0.arrow\n",
            "Processing file 1ur0AHTqFqHOZehQAWmfU6m_FTW9eVhi5 cache-11ec14a2b8592ef5.arrow\n",
            "Processing file 18b1phZ93UL7L1q1_dryZVEn3KGQVjTHo dataset_info.json\n",
            "Processing file 1Sx7oFThX8WCdv9QvoRfdtupIMUxU6hrE dataset.arrow\n",
            "Processing file 1HBme-Iy_m1_rPxgaPCblL9AU4IBKUWCr state.json\n",
            "Processing file 1ibg1gDmdcHqqhoKxWSxTyp6GNA7EsP9T dataset_dict.json\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14ptaG9y6X6uwYf4GqRZkiT5XxyV4uoId\n",
            "To: /content/drive/MyDrive/sentiment_analysis_data/amazon_multilingual/de/canine-s-finetuned/canine-s_best_model.pt\n",
            "100% 529M/529M [00:01<00:00, 266MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mDbXDvyW1YbeiA0NROqwRdgOOF6UfEp9\n",
            "To: /content/drive/MyDrive/sentiment_analysis_data/amazon_multilingual/de/test/cache-0cf405bf6d7327dc.arrow\n",
            "100% 50.1M/50.1M [00:00<00:00, 102MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LLSS0fRy6u4TRY-Fmxl-rUIHZIBF6Lm2\n",
            "To: /content/drive/MyDrive/sentiment_analysis_data/amazon_multilingual/de/test/cache-0ff9c1dea035819f.arrow\n",
            "100% 11.2M/11.2M [00:00<00:00, 159MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_PkU68qOcd9vxyrSQWVPPbu1VVTZbgjI\n",
            "To: /content/drive/MyDrive/sentiment_analysis_data/amazon_multilingual/de/test/cache-30e7cd049319cfd2.arrow\n",
            "100% 11.2M/11.2M [00:00<00:00, 249MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qVp6kMeoZsrbgntO5vBYPIhGwqXBTvMh\n",
            "To: /content/drive/MyDrive/sentiment_analysis_data/amazon_multilingual/de/test/cache-799e77ac4d257bac.arrow\n",
            "100% 13.2M/13.2M [00:00<00:00, 218MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=139dxZmgiWy0LkHm7422Myw4aMQkrd2N7\n",
            "To: /content/drive/MyDrive/sentiment_analysis_data/amazon_multilingual/de/test/cache-ab14ae5ad6b924d0.arrow\n",
            "100% 50.1M/50.1M [00:00<00:00, 83.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-6kVANED6LPTnLwWTbS5npvES36C4z-7\n",
            "To: /content/drive/MyDrive/sentiment_analysis_data/amazon_multilingual/de/test/cache-ffca98777b678b04.arrow\n",
            "100% 50.1M/50.1M [00:00<00:00, 250MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1D4teKKM1bF4imSVnIg3Cb7fkwEozgdCy\n",
            "To: /content/drive/MyDrive/sentiment_analysis_data/amazon_multilingual/de/test/dataset_info.json\n",
            "100% 574/574 [00:00<00:00, 1.53MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Mu-GetN15oWscZosSXNbR20NcDBbSS9E\n",
            "To: /content/drive/MyDrive/sentiment_analysis_data/amazon_multilingual/de/test/dataset.arrow\n",
            "100% 880k/880k [00:00<00:00, 119MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AzJd6PboyfyOAIO68cqtGzXBgiI1s2Wh\n",
            "To: /content/drive/MyDrive/sentiment_analysis_data/amazon_multilingual/de/test/state.json\n",
            "100% 253/253 [00:00<00:00, 700kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PPBNEj6ftdofsYuQ59M_J85yP1PSs2mS\n",
            "To: /content/drive/MyDrive/sentiment_analysis_data/amazon_multilingual/de/train/cache-3e52fa294a49ee1a.arrow\n",
            "100% 2.00G/2.00G [00:07<00:00, 264MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KasXe8jYFDRE6iIonx_qOUlH9Xp7BgCx\n",
            "To: /content/drive/MyDrive/sentiment_analysis_data/amazon_multilingual/de/train/cache-8ddeb733db03c67b.arrow\n",
            "100% 2.00G/2.00G [00:07<00:00, 268MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18rmCVT7nnqBUCoZkVoLefWfglRzB7AQN\n",
            "To: /content/drive/MyDrive/sentiment_analysis_data/amazon_multilingual/de/train/cache-43de474c5b12fe18.arrow\n",
            "100% 2.00G/2.00G [00:29<00:00, 67.4MB/s]\n",
            "Access denied with the following error:\n",
            "\n",
            " \tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1guryQJn2CRoSoKOgYRLXPKqyt7QRgIJT \n",
            "\n",
            "Download ended unsuccessfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cd into the question_answering folder to access the Python package\n",
        "\n",
        "%cd nlp_ensae/source/sentiment_analysis/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDm9absuuDVE",
        "outputId": "90a462f2-69f0-4b6c-e6d5-344090f5dff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nlp_ensae/source/sentiment_analysis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Ch1m82rZY44z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "import logging\n",
        "import os\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Any, Dict, List, OrderedDict, Callable, Tuple, Union, Optional\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset, load_metric, load_from_disk, DatasetDict, Dataset\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers.data.data_collator import InputDataClass\n",
        "from transformers.trainer_utils import PredictionOutput, EvalPrediction\n",
        "\n",
        "from nlpaug import Augmenter\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from datasets import load_metric, load_from_disk\n",
        "from transformers import (\n",
        "    CanineTokenizer,\n",
        "    IntervalStrategy,\n",
        "    RobertaTokenizerFast,\n",
        "    SchedulerType,\n",
        "    BertTokenizerFast,\n",
        "    DistilBertTokenizerFast,\n",
        "    XLMRobertaTokenizerFast,\n",
        "    DataCollatorWithPadding,\n",
        "    CanineForSequenceClassification,\n",
        "    BertForSequenceClassification,\n",
        "    RobertaForSequenceClassification,\n",
        "    DistilBertForSequenceClassification,\n",
        "    PretrainedConfig,\n",
        "    PreTrainedTokenizer,\n",
        "    BatchEncoding,\n",
        ")\n",
        "\n",
        "from sentiment_analysis import (\n",
        "    DatasetTokenizer,\n",
        "    set_seed,\n",
        "    TrainerArguments,\n",
        "    DataArguments,\n",
        "    CustomTrainer,\n",
        "    save_predictions_to_pandas_dataframe,\n",
        "    Model,\n",
        ")\n"
      ],
      "metadata": {
        "id": "ogKJNCt-Y5n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caveats\n",
        "\n",
        "The code developped for the Sentiment Classification task was mainly made to be run on a remote server and not on a jupyter notebook, which is why the following notebook is mostly made of bash command. To see more in details the code in itself, we strongly advise you to look at the package and the corresponding `README.md` to get more information.\n",
        "\n",
        "However, we will show some core classes in the following cell so that you might get a better feeling of what is going on."
      ],
      "metadata": {
        "id": "LW9JugP8sYw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parts of the Python package"
      ],
      "metadata": {
        "id": "mww-klLMesVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### main.py script"
      ],
      "metadata": {
        "id": "ylJbEcq6x-NQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_LABELS = 2\n",
        "\n",
        "SEED = 0\n",
        "set_seed(SEED)\n",
        "\n",
        "CANINE_S_MODEL = \"canine-s\"\n",
        "CANINE_C_MODEL = \"canine-c\"\n",
        "BERT_MODEL = \"bert\"\n",
        "MBERT_MODEL = \"mbert\"\n",
        "XLM_ROBERTA_MODEL = \"xlm_roberta\"\n",
        "ROBERTA_MODEL = \"roberta\"\n",
        "DISTILBERT_MODEL = \"distilbert\"\n",
        "\n",
        "SST2_DATASET_CONFIG = \"sst2\"\n",
        "GLUE_DATASET_NAME = \"glue\"\n",
        "SENT140_DATASET_NAME = \"sentiment140\"\n",
        "AMAZON_MULTI_DATASET_NAME = \"amazon_reviews_multi\"\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model_name: str,\n",
        "    learning_rate: float,\n",
        "    weight_decay: float,\n",
        "    type_lr_scheduler: SchedulerType,\n",
        "    warmup_ratio: float,\n",
        "    save_strategy: IntervalStrategy,\n",
        "    save_steps: int,\n",
        "    num_epochs: int,\n",
        "    early_stopping_patience: int,\n",
        "    output_dir: str,\n",
        "    device: str,\n",
        "    dataset_name: str,\n",
        "    batch_size: int,\n",
        "    truncation: bool,\n",
        "    eval_only: bool,\n",
        "    path_to_finetuned_model: str,\n",
        "    dataset_config: str,\n",
        "    padding: str,\n",
        "    path_to_custom_dataset: str,\n",
        "    mode: str,\n",
        "    save_predictions: bool,\n",
        ") -> None:\n",
        "    logger.info(f\"Loading dataset {dataset_name}\")\n",
        "    if dataset_name == GLUE_DATASET_NAME:\n",
        "        logger.info(f\"Chosen configuration is {dataset_config}\")\n",
        "        datasets = load_from_disk(path_to_custom_dataset)\n",
        "    elif dataset_name == SENT140_DATASET_NAME:\n",
        "        datasets = load_from_disk(path_to_custom_dataset)\n",
        "    elif dataset_name == AMAZON_MULTI_DATASET_NAME:\n",
        "        datasets = load_from_disk(path_to_custom_dataset)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    print(datasets)\n",
        "    logger.info(f\"Preparing for model {model_name}\")\n",
        "    if model_name in [CANINE_C_MODEL, CANINE_S_MODEL]:\n",
        "        pretrained_model_name = f\"google/{model_name}\"\n",
        "        tokenizer = CanineTokenizer.from_pretrained(pretrained_model_name)\n",
        "        model = CanineForSequenceClassification.from_pretrained(\n",
        "            pretrained_model_name, num_labels=NUM_LABELS\n",
        "        )\n",
        "    else:\n",
        "        if model_name == BERT_MODEL:\n",
        "            pretrained_model_name = \"bert-base-uncased\"\n",
        "            tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name)\n",
        "            model = BertForSequenceClassification.from_pretrained(\n",
        "                pretrained_model_name, num_labels=NUM_LABELS\n",
        "            )\n",
        "\n",
        "        elif model_name == MBERT_MODEL:\n",
        "            pretrained_model_name = \"bert-base-multilingual-cased\"\n",
        "            tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name)\n",
        "            model = BertForSequenceClassification.from_pretrained(\n",
        "                pretrained_model_name, num_labels=NUM_LABELS\n",
        "            )\n",
        "\n",
        "        elif model_name == XLM_ROBERTA_MODEL:\n",
        "            pretrained_model_name = \"xlm-roberta-base\"\n",
        "            tokenizer = XLMRobertaTokenizerFast.from_pretrained(pretrained_model_name)\n",
        "            model = RobertaForSequenceClassification.from_pretrained(\n",
        "                pretrained_model_name, num_labels=NUM_LABELS\n",
        "            )\n",
        "\n",
        "        elif model_name == ROBERTA_MODEL:\n",
        "            pretrained_model_name = \"roberta-base\"\n",
        "            tokenizer = RobertaTokenizerFast.from_pretrained(pretrained_model_name)\n",
        "            model = RobertaForSequenceClassification.from_pretrained(\n",
        "                pretrained_model_name, num_labels=NUM_LABELS\n",
        "            )\n",
        "\n",
        "        elif model_name == DISTILBERT_MODEL:\n",
        "            pretrained_model_name = \"distilbert-base-uncased\"\n",
        "            tokenizer = DistilBertTokenizerFast.from_pretrained(pretrained_model_name)\n",
        "            model = DistilBertForSequenceClassification.from_pretrained(\n",
        "                pretrained_model_name, num_labels=NUM_LABELS\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    dataset_tokenizer = DatasetTokenizer(\n",
        "        tokenizer=tokenizer,\n",
        "        padding=padding,\n",
        "        truncation=truncation,\n",
        "    )\n",
        "\n",
        "    logger.info(\"Tokenizing dataset\")\n",
        "    tokenized_datasets = datasets.map(\n",
        "        dataset_tokenizer.tokenize,\n",
        "        batched=True,\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer, padding=padding)\n",
        "    metric = load_metric(\"glue\", \"sst2\")\n",
        "\n",
        "    if eval_only:\n",
        "        logger.info(\"Loading own finetuned model\")\n",
        "        model.load_state_dict(torch.load(path_to_finetuned_model, map_location=device))\n",
        "\n",
        "    trainer_args = TrainerArguments(\n",
        "        model=model,\n",
        "        learning_rate=learning_rate,\n",
        "        lr_scheduler=type_lr_scheduler,\n",
        "        warmup_ratio=warmup_ratio,\n",
        "        save_strategy=save_strategy,\n",
        "        save_steps=save_steps,\n",
        "        epochs=num_epochs,\n",
        "        output_dir=output_dir,\n",
        "        metric=metric,\n",
        "        evaluation_strategy=save_strategy,\n",
        "        weight_decay=weight_decay,\n",
        "        data_collator=data_collator,\n",
        "        model_save_path=os.path.join(\n",
        "            output_dir, f\"{model_name}-finetuned\", f\"{model_name}_best_model.pt\"\n",
        "        ),\n",
        "        device=device,\n",
        "        early_stopping_patience=early_stopping_patience,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "    )\n",
        "\n",
        "    data_args = DataArguments(\n",
        "        datasets=datasets,\n",
        "        dataset_name=dataset_name,\n",
        "        dataset_config=dataset_config,\n",
        "        batch_size=batch_size,\n",
        "        tokenizer=tokenizer,\n",
        "        tokenized_datasets=tokenized_datasets,\n",
        "    )\n",
        "\n",
        "    if model_name in [\n",
        "        CANINE_C_MODEL,\n",
        "        CANINE_S_MODEL,\n",
        "        BERT_MODEL,\n",
        "        MBERT_MODEL,\n",
        "        XLM_ROBERTA_MODEL,\n",
        "        ROBERTA_MODEL,\n",
        "        DISTILBERT_MODEL,\n",
        "    ]:\n",
        "        trainer = CustomTrainer(trainer_args, data_args, model_name)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # check if we are in eval mode only or not\n",
        "    if not eval_only:\n",
        "        logger.info(\"START TRAINING\")\n",
        "        trainer.train()\n",
        "\n",
        "    logger.info(\"START FINAL EVALUATION\")\n",
        "    trainer.evaluate()\n",
        "    logger.info(\"Final evaluation done\")\n",
        "\n",
        "    logger.info(\"GET PREDICTIONS\")\n",
        "    test_predictions = trainer.predict(mode=mode)\n",
        "    logger.info(\"Predictions done\")\n",
        "    results = trainer.evaluate_predictions(test_predictions)\n",
        "    logger.info(f\"Predictions accuracy {results['accuracy']}\")\n",
        "    if save_predictions:\n",
        "        save_predictions_to_pandas_dataframe(\n",
        "            test_predictions,\n",
        "            datasets,\n",
        "            output_dir,\n",
        "            model_name,\n",
        "            mode,\n",
        "            logger,\n",
        "        )\n",
        "\n",
        "    if not eval_only:\n",
        "        # Save best model\n",
        "        trainer.save_model()\n"
      ],
      "metadata": {
        "id": "40Bpq8r0uB73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    debug = False\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    logging.getLogger(\"datasets\").setLevel(logging.ERROR)\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Parser for training and data arguments\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--model_name\",\n",
        "        type=str,\n",
        "        help=\"Name of the model\",\n",
        "        choices=[\n",
        "            MBERT_MODEL,\n",
        "            BERT_MODEL,\n",
        "            CANINE_S_MODEL,\n",
        "            CANINE_C_MODEL,\n",
        "            ROBERTA_MODEL,\n",
        "            XLM_ROBERTA_MODEL,\n",
        "            DISTILBERT_MODEL,\n",
        "        ],\n",
        "        required=True,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--learning_rate\",\n",
        "        type=float,\n",
        "        required=True,\n",
        "        help=\"Chosen learning rate for AdamW optimizer\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--weight_decay\",\n",
        "        type=float,\n",
        "        required=True,\n",
        "        help=\"Chosen weight decay for AdamW optimizer\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--type_lr_scheduler\", type=str, required=True, help=\"Type of LR scheduler\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--warmup_ratio\", type=float, required=True, help=\"Warmup ratio\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--save_strategy\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Save strategy\",\n",
        "        choices=[\"steps\", \"epochs\"],\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--save_steps\",\n",
        "        type=int,\n",
        "        required=True,\n",
        "        help=\"Number of steps to perform before saving model\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num_epochs\", type=int, required=True, help=\"Number of epochs to train for\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--early_stopping_patience\",\n",
        "        type=int,\n",
        "        required=True,\n",
        "        help=\"Patience for early stopping, validation loss is monitored\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\", type=str, required=True, help=\"Directory to store the model\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--device\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"Device to run the code on, either cpu or cuda\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--dataset_name\",\n",
        "        type=str,\n",
        "        default=\"glue\",\n",
        "        choices=[GLUE_DATASET_NAME, SENT140_DATASET_NAME, AMAZON_MULTI_DATASET_NAME],\n",
        "        required=True,\n",
        "        help=\"Name of the dataset to train/evaluate on\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--dataset_config\",\n",
        "        type=str,\n",
        "        default=\"sst2\",\n",
        "        help=\"Config name for GLUE dataset\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--batch_size\",\n",
        "        type=int,\n",
        "        required=True,\n",
        "        help=\"Batch size for training and evaluation\",\n",
        "    )\n",
        "    parser.add_argument(\"--eval_only\", type=bool, default=False)\n",
        "    parser.add_argument(\n",
        "        \"--path_to_finetuned_model\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"Path towards a previously finetuned model\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--truncation\",\n",
        "        type=bool,\n",
        "        required=True,\n",
        "        help=\"Whether or not tokenizer should truncate the inputs\",\n",
        "    )\n",
        "    parser.add_argument(\"--padding\", type=str, help=\"Padding strategy\")\n",
        "    parser.add_argument(\n",
        "        \"--path_to_custom_dataset\",\n",
        "        type=str,\n",
        "        help=\"Path to custom dataset\",\n",
        "        required=True,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--mode\",\n",
        "        type=str,\n",
        "        help=\"Evaluation mode\",\n",
        "        choices=[\"val\", \"test\"],\n",
        "        required=True,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--save_predictions\",\n",
        "        type=bool,\n",
        "        help=\"Whether or not to save the predictions into a csv file\",\n",
        "        default=False,\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_model(\n",
        "        model_name=args.model_name,\n",
        "        learning_rate=args.learning_rate,\n",
        "        weight_decay=args.weight_decay,\n",
        "        type_lr_scheduler=args.type_lr_scheduler,\n",
        "        warmup_ratio=args.warmup_ratio,\n",
        "        save_strategy=args.save_strategy,\n",
        "        save_steps=args.save_steps,\n",
        "        num_epochs=args.num_epochs,\n",
        "        early_stopping_patience=args.early_stopping_patience,\n",
        "        output_dir=args.output_dir,\n",
        "        device=args.device,\n",
        "        dataset_name=args.dataset_name,\n",
        "        batch_size=args.batch_size,\n",
        "        truncation=args.truncation,\n",
        "        eval_only=args.eval_only,\n",
        "        path_to_finetuned_model=args.path_to_finetuned_model,\n",
        "        dataset_config=args.dataset_config,\n",
        "        padding=args.padding,\n",
        "        path_to_custom_dataset=args.path_to_custom_dataset,\n",
        "        mode=args.mode,\n",
        "        save_predictions=args.save_predictions,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "ee6JsbD5Fk_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CANINE model"
      ],
      "metadata": {
        "id": "qLxzT6fUyBXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HuggingFaceModelT = Any\n",
        "CANINE_C = \"google/canine-c\"\n",
        "CANINE_S = \"google/canine-s\"\n",
        "\n",
        "class Model(nn.Module):\n",
        "    \"\"\"Generic model for Sentiment Classification Tasks\"\"\"\n",
        "\n",
        "    def __init__(self, model: HuggingFaceModelT, config: PretrainedConfig):\n",
        "        nn.Module.__init__(self)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.model = model\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor = None,\n",
        "        attention_mask: torch.Tensor = None,\n",
        "        token_type_ids: torch.Tensor = None,\n",
        "        position_ids: torch.Tensor = None,\n",
        "        head_mask: torch.Tensor = None,\n",
        "        inputs_embeds: torch.Tensor = None,\n",
        "        output_attentions: torch.Tensor = None,\n",
        "        output_hidden_states: torch.Tensor = None,\n",
        "        return_dict: bool = None,\n",
        "    ):\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "\n",
        "class CanineSA(Model):\n",
        "    \"\"\"CANINE model for Sentiment Classification Tasks\"\"\"\n",
        "\n",
        "    def __init__(self, pretrained_model_name: str = Union[CANINE_C, CANINE_S]) -> None:\n",
        "        config = CanineConfig(num_labels=2)\n",
        "        canine = CanineModel.from_pretrained(pretrained_model_name)\n",
        "        Model.__init__(self, canine, config)\n"
      ],
      "metadata": {
        "id": "0YjuDHOeyHA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset Tokenizer\n",
        "\n",
        "Class to tokenize the dataset and provide right inputs for the model."
      ],
      "metadata": {
        "id": "t71Qe4bsyk-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetTokenizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        padding: str,\n",
        "        truncation: bool,\n",
        "    ) -> None:\n",
        "        self.tokenizer = tokenizer\n",
        "        self.padding = padding\n",
        "        self.truncation = truncation\n",
        "\n",
        "    def tokenize(self, data: Dataset) -> BatchEncoding:\n",
        "        return self.tokenizer(\n",
        "            data[\"sentence\"],\n",
        "            padding=self.padding,\n",
        "            truncation=self.truncation,\n",
        "        )"
      ],
      "metadata": {
        "id": "Y0bNXnbjyky-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Base Custom Trainer"
      ],
      "metadata": {
        "id": "pIaPgHCzFv9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TrainerArguments:\n",
        "    \"\"\"\n",
        "    Arguments needed to initiate a Trainer\n",
        "    \"\"\"\n",
        "\n",
        "    model: nn.Module\n",
        "    learning_rate: float\n",
        "    lr_scheduler: SchedulerType\n",
        "    warmup_ratio: float\n",
        "    save_strategy: IntervalStrategy\n",
        "    save_steps: int\n",
        "    epochs: int\n",
        "    output_dir: str\n",
        "    metric: Any\n",
        "    evaluation_strategy: IntervalStrategy\n",
        "    weight_decay: float\n",
        "    data_collator: Callable[[List[InputDataClass]], Dict[str, Any]]\n",
        "    model_save_path: str\n",
        "    device: str\n",
        "    early_stopping_patience: int\n",
        "    metric_for_best_model: str\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    \"\"\"\n",
        "    Data arguments needed to initiate a Trainer\n",
        "    \"\"\"\n",
        "\n",
        "    datasets: DatasetDict\n",
        "    dataset_name: str\n",
        "    dataset_config: str\n",
        "    batch_size: int\n",
        "    tokenizer: PreTrainedTokenizer\n",
        "    tokenized_datasets: DatasetDict\n",
        "\n",
        "\n",
        "class CustomTrainer(ABC):\n",
        "    \"\"\"General Trainer signature\"\"\"\n",
        "\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    def __init__(\n",
        "        self, trainer_args: TrainerArguments, data_args: DataArguments, model_name: str\n",
        "    ) -> None:\n",
        "        self.trainer_args = trainer_args\n",
        "        self.data_args = data_args\n",
        "        self.model_name = model_name\n",
        "\n",
        "        # Define training arguments\n",
        "        args = TrainingArguments(\n",
        "            output_dir=os.path.join(\n",
        "                self.trainer_args.output_dir, self.model_name + \"-finetuned\"\n",
        "            ),\n",
        "            evaluation_strategy=self.trainer_args.evaluation_strategy,\n",
        "            learning_rate=self.trainer_args.learning_rate,\n",
        "            weight_decay=self.trainer_args.weight_decay,\n",
        "            num_train_epochs=self.trainer_args.epochs,\n",
        "            lr_scheduler_type=self.trainer_args.lr_scheduler,\n",
        "            warmup_ratio=self.trainer_args.warmup_ratio,\n",
        "            per_device_train_batch_size=self.data_args.batch_size,\n",
        "            per_device_eval_batch_size=self.data_args.batch_size,\n",
        "            save_strategy=self.trainer_args.save_strategy,\n",
        "            save_steps=self.trainer_args.save_steps,\n",
        "            push_to_hub=False,\n",
        "            metric_for_best_model=trainer_args.metric_for_best_model,\n",
        "            load_best_model_at_end=True,\n",
        "            logging_steps=self.trainer_args.save_steps,\n",
        "            no_cuda=False if self.trainer_args.device == \"cuda\" else True,\n",
        "        )\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStoppingCallback(\n",
        "                early_stopping_patience=self.trainer_args.early_stopping_patience\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        self.trainer = Trainer(\n",
        "            self.trainer_args.model,\n",
        "            args,\n",
        "            train_dataset=self.data_args.tokenized_datasets[\"train\"],\n",
        "            eval_dataset=self.data_args.tokenized_datasets[\"validation\"],\n",
        "            data_collator=self.trainer_args.data_collator,\n",
        "            tokenizer=self.data_args.tokenizer,\n",
        "            callbacks=callbacks,\n",
        "            compute_metrics=self._compute_metrics,\n",
        "        )\n",
        "\n",
        "    def train(self) -> None:\n",
        "        self.logger.info(\"Start training\")\n",
        "        self.trainer.train()\n",
        "        self.logger.info(\"Training done\")\n",
        "\n",
        "    def evaluate(self) -> None:\n",
        "        self.logger.info(\"Start evaluation\")\n",
        "        results = self.trainer.evaluate()\n",
        "        self.logger.info(\n",
        "            f\"Evaluation done: Eval loss {results['eval_loss']}, Eval accuracy {results['eval_accuracy']}\"\n",
        "        )\n",
        "\n",
        "    def predict(self, mode: str) -> PredictionOutput:\n",
        "        self.logger.info(f\"Start predicting on {mode} set\")\n",
        "        if mode == \"val\":\n",
        "            data = self.data_args.tokenized_datasets[\"validation\"]\n",
        "        elif mode == \"test\":\n",
        "            data = self.data_args.tokenized_datasets[\"test\"]\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        predictions = self.trainer.predict(data)\n",
        "        self.logger.info(\"Prediction done\")\n",
        "        return predictions\n",
        "\n",
        "    def evaluate_predictions(\n",
        "        self, eval_predictions: PredictionOutput\n",
        "    ) -> Dict[str, float]:\n",
        "        return self._compute_metrics(eval_predictions)\n",
        "\n",
        "    def save_model(self) -> None:\n",
        "        self.logger.info(\n",
        "            f\"Saving best trained model at {self.trainer_args.model_save_path}\"\n",
        "        )\n",
        "        torch.save(self.trainer.model.state_dict(), self.trainer_args.model_save_path)\n",
        "\n",
        "    def _compute_metrics(\n",
        "        self, eval_predictions: Union[EvalPrediction, PredictionOutput]\n",
        "    ) -> Dict[str, float]:\n",
        "        try:\n",
        "            predictions, labels = eval_predictions\n",
        "        except:\n",
        "            predictions, labels = (\n",
        "                eval_predictions.predictions,\n",
        "                eval_predictions.label_ids,\n",
        "            )\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        return self.trainer_args.metric.compute(\n",
        "            predictions=predictions, references=labels\n",
        "        )"
      ],
      "metadata": {
        "id": "G5dLwC6gFyPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CLI commands to finetune models on SST2 \n",
        "\n",
        "Following examples are given for RoBERTa and CANINE-S models but can be applied also to:\n",
        "\n",
        "- BERT\n",
        "- mBERT\n",
        "- DistilBERT\n",
        "- XLM-RoBERTa\n",
        "- CANINE-C"
      ],
      "metadata": {
        "id": "4dFcY4VAEr9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "!python3 main.py \\\n",
        "    --model_name canine-s \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 1e-2 \\\n",
        "    --type_lr_scheduler linear \\\n",
        "    --warmup_ratio 0.1 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_steps 2500 \\\n",
        "    --num_epochs 3 \\\n",
        "    --early_stopping_patience 1 \\\n",
        "    --output_dir /content/drive/MyDrive/sentiment_analysis \\\n",
        "    --dataset_name glue \\\n",
        "    --dataset_config sst2 \\\n",
        "    --batch_size 12 \\\n",
        "    --truncation True \\\n",
        "    --padding max_length \\\n",
        "    --path_to_custom_dataset /content/drive/MyDrive/sentiment_analysis_data/sst2 \\\n",
        "    --mode test \\\n",
        "    --device cuda \\\n"
      ],
      "metadata": {
        "id": "FHPel8KTuCDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "!python3 main.py \\\n",
        "    --model_name xlm_roberta \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 1e-2 \\\n",
        "    --type_lr_scheduler linear \\\n",
        "    --warmup_ratio 0.1 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_steps 2500 \\\n",
        "    --num_epochs 3 \\\n",
        "    --early_stopping_patience 1 \\\n",
        "    --output_dir /content/drive/MyDrive/sentiment_analysis \\\n",
        "    --dataset_name glue \\\n",
        "    --dataset_config sst2 \\\n",
        "    --batch_size 12 \\\n",
        "    --truncation True \\\n",
        "    --padding max_length \\\n",
        "    --path_to_custom_dataset /content/drive/MyDrive/sentiment_analysis_data/sst2 \\\n",
        "    --mode test \\\n",
        "    --device cuda \\\n"
      ],
      "metadata": {
        "id": "b5pYq_RzuCK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CLI commands to do zero-shot transfer learning and domain adaptation from SST2 to Sentiment140\n"
      ],
      "metadata": {
        "id": "1FzvSey-K7Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%script false --no-raise-error\n",
        "\n",
        "!python3 main.py \\\n",
        "    --model_name canine-s \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 1e-2 \\\n",
        "    --type_lr_scheduler linear \\\n",
        "    --warmup_ratio 0.1 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_steps 2500 \\\n",
        "    --num_epochs 3 \\\n",
        "    --early_stopping_patience 1 \\\n",
        "    --output_dir /content/drive/MyDrive/sentiment_analysis \\\n",
        "    --dataset_name sentiment140 \\\n",
        "    --batch_size 6 \\\n",
        "    --truncation True \\\n",
        "    --padding max_length \\\n",
        "    --path_to_custom_dataset /content/drive/MyDrive/sentiment_analysis_data/sentiment_analysis_140/sentiment140 \\\n",
        "    --mode test \\\n",
        "    --eval_only True \\\n",
        "    --device cuda \\\n",
        "    --path_to_finetuned_model /content/drive/MyDrive/sentiment_analysis/canine-s-finetuned/canine-s_best_model.pt"
      ],
      "metadata": {
        "id": "Nb6Tfcg5L14e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "347f21e9-7de1-4429-eabf-3792cc2985c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Loading dataset sentiment140\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence', 'labels'],\n",
            "        num_rows: 63360\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence', 'labels'],\n",
            "        num_rows: 359\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence', 'labels'],\n",
            "        num_rows: 16000\n",
            "    })\n",
            "})\n",
            "INFO:__main__:Preparing for model canine-s\n",
            "Downloading: 100% 657/657 [00:00<00:00, 628kB/s]\n",
            "Downloading: 100% 854/854 [00:00<00:00, 835kB/s]\n",
            "Downloading: 100% 670/670 [00:00<00:00, 579kB/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Downloading: 100% 504M/504M [00:08<00:00, 63.0MB/s]\n",
            "Some weights of CanineForSequenceClassification were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:__main__:Tokenizing dataset\n",
            "Downloading builder script: 5.76kB [00:00, 7.65MB/s]       \n",
            "INFO:__main__:Loading own finetuned model\n",
            "INFO:__main__:START FINAL EVALUATION\n",
            "INFO:sentiment_analysis.trainers.trainer:Start evaluation\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `CanineForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `CanineForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16000\n",
            "  Batch size = 6\n",
            "100% 2667/2667 [06:12<00:00,  7.16it/s]\n",
            "INFO:sentiment_analysis.trainers.trainer:Evaluation done: Eval loss 1.9509079456329346, Eval accuracy 0.6485\n",
            "INFO:__main__:Final evaluation done\n",
            "INFO:__main__:GET PREDICTIONS\n",
            "INFO:sentiment_analysis.trainers.trainer:Start predicting on test set\n",
            "The following columns in the test set  don't have a corresponding argument in `CanineForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `CanineForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 359\n",
            "  Batch size = 6\n",
            "100% 60/60 [00:08<00:00,  7.56it/s]INFO:sentiment_analysis.trainers.trainer:Prediction done\n",
            "INFO:__main__:Predictions done\n",
            "INFO:__main__:Predictions accuracy 0.7298050139275766\n",
            "100% 60/60 [00:08<00:00,  7.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%script false --no-raise-error\n",
        "\n",
        "!python3 main.py \\\n",
        "    --model_name bert \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 1e-2 \\\n",
        "    --type_lr_scheduler linear \\\n",
        "    --warmup_ratio 0.1 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_steps 2500 \\\n",
        "    --num_epochs 3 \\\n",
        "    --early_stopping_patience 1 \\\n",
        "    --output_dir /content/drive/MyDrive/sentiment_analysis \\\n",
        "    --dataset_name sentiment140 \\\n",
        "    --batch_size 6 \\\n",
        "    --truncation True \\\n",
        "    --padding max_length \\\n",
        "    --path_to_custom_dataset /content/drive/MyDrive/sentiment_analysis_data/sentiment_analysis_140/sentiment140 \\\n",
        "    --mode test \\\n",
        "    --eval_only True \\\n",
        "    --device cuda \\\n",
        "    --path_to_finetuned_model /content/drive/MyDrive/sentiment_analysis/bert-finetuned/bert_best_model.pt"
      ],
      "metadata": {
        "id": "NQCfoPYqLCwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e55a64-65ee-4fe1-b6ff-cdb22263cc74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Loading dataset sentiment140\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence', 'labels'],\n",
            "        num_rows: 63360\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence', 'labels'],\n",
            "        num_rows: 359\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence', 'labels'],\n",
            "        num_rows: 16000\n",
            "    })\n",
            "})\n",
            "INFO:__main__:Preparing for model bert\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 28.5kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 321kB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 517kB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 543kB/s]\n",
            "Downloading: 100% 420M/420M [00:06<00:00, 72.3MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:__main__:Tokenizing dataset\n",
            "100% 64/64 [00:12<00:00,  5.25ba/s]\n",
            "100% 1/1 [00:00<00:00, 14.54ba/s]\n",
            "100% 16/16 [00:03<00:00,  5.05ba/s]\n",
            "INFO:__main__:Loading own finetuned model\n",
            "INFO:__main__:START FINAL EVALUATION\n",
            "INFO:sentiment_analysis.trainers.trainer:Start evaluation\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 16000\n",
            "  Batch size = 6\n",
            "100% 2667/2667 [02:55<00:00, 15.22it/s]\n",
            "INFO:sentiment_analysis.trainers.trainer:Evaluation done: Eval loss 1.225546956062317, Eval accuracy 0.7224375\n",
            "INFO:__main__:Final evaluation done\n",
            "INFO:__main__:GET PREDICTIONS\n",
            "INFO:sentiment_analysis.trainers.trainer:Start predicting on test set\n",
            "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 359\n",
            "  Batch size = 6\n",
            "100% 60/60 [00:03<00:00, 15.67it/s]INFO:sentiment_analysis.trainers.trainer:Prediction done\n",
            "INFO:__main__:Predictions done\n",
            "INFO:__main__:Predictions accuracy 0.841225626740947\n",
            "100% 60/60 [00:04<00:00, 14.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code used to generate noisy datasets\n",
        "\n",
        "Noise generated on SST2 dataset."
      ],
      "metadata": {
        "id": "xbpsU2ETzBxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "from datasets import load_from_disk, DatasetDict, Dataset\n",
        "from nlpaug import Augmenter\n",
        "from transformers import HfArgumentParser\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class NoisifierArguments:\n",
        "    \"\"\"\n",
        "    Arguments needed to noisify SST2-like datasets.\n",
        "    \"\"\"\n",
        "\n",
        "    path_to_custom_dataset: str = field(\n",
        "        default=None, metadata={\"help\": \"Path towards custom dataset\"}\n",
        "    )\n",
        "    dataset_name: str = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Name of the dataset. Either sst2 or sentiment140\"},\n",
        "    )\n",
        "    output_dir: str = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"Output directory, will be used to store noisy dataset in csv format\"\n",
        "        },\n",
        "    )\n",
        "    noise_level: float = field(\n",
        "        default=None, metadata={\"help\": \"Level of noise to apply on the dataset\"}\n",
        "    )\n",
        "    augmenter_type: str = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"Type of Augmenter to use. Either: KeyboardAug, RandomCharAug, SpellingAug, BackTranslationAug \"\n",
        "            \"(de/en) or OcrAug\"\n",
        "        },\n",
        "    )\n",
        "    action: str = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"Type of action to apply if RandomCharAug was chosen. Either: swap, substitute, insert or delete.\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        if (self.augmenter_type == \"RandomCharAug\" and self.action is None) or (\n",
        "            self.action is not None and self.augmenter_type != \"RandomCharAug\"\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                \"If you set `augmenter_type` to RandomCharAug, please choose an `action`.\"\n",
        "                \"If you've chosen an `action`, you must choose `augmenter_type`==RandomCharAug for it\"\n",
        "                \"to work.\"\n",
        "            )\n",
        "\n",
        "\n",
        "class Noisifier:\n",
        "    def __init__(\n",
        "        self,\n",
        "        datasets: DatasetDict,\n",
        "        dataset_name: str,\n",
        "        level: float,\n",
        "        type: str,\n",
        "        action: Optional[str],\n",
        "    ) -> None:\n",
        "        self.datasets = datasets\n",
        "        self.dataset_name = dataset_name\n",
        "        self.level = level\n",
        "        self.type = type\n",
        "        self.action = action\n",
        "\n",
        "    def _get_augmenter(self) -> Augmenter:\n",
        "        if self.type == \"KeyboardAug\":\n",
        "            return nac.KeyboardAug()\n",
        "\n",
        "        elif self.type == \"RandomCharAug\":\n",
        "            return nac.RandomCharAug(action=self.action)\n",
        "\n",
        "        elif self.type == \"SpellingAug\":\n",
        "            return naw.SpellingAug()\n",
        "\n",
        "        elif self.type == \"OcrAug\":\n",
        "            return nac.OcrAug()\n",
        "\n",
        "        elif self.type == \"BackTranslationAug\":\n",
        "            return naw.BackTranslationAug(\n",
        "                from_model_name=\"facebook/wmt19-en-de\",\n",
        "                to_model_name=\"facebook/wmt19-de-en\",\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def _augment_text(self, row: Dataset) -> Dataset:\n",
        "        if self.dataset_name == \"sst2\":\n",
        "            text = \"sentence\"\n",
        "        elif self.dataset_name == \"sentiment140\":\n",
        "            text = \"text\"\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        augmenter = self._get_augmenter()\n",
        "        if random.random() < self.level:\n",
        "            row[text] = augmenter.augment(row[text])\n",
        "        return row\n",
        "\n",
        "    def augment(self):\n",
        "        if (\n",
        "            \"train\" in self.datasets.column_names\n",
        "            and \"validation\" in self.datasets.column_names\n",
        "            and \"test\" in self.datasets.column_names\n",
        "        ):\n",
        "            self.datasets[\"train\"] = self.datasets[\"train\"].map(self._augment_text)\n",
        "            self.datasets[\"validation\"] = self.datasets[\"validation\"].map(\n",
        "                self._augment_text\n",
        "            )\n",
        "            self.datasets[\"test\"] = self.datasets[\"test\"].map(self._augment_text)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        return self.datasets\n"
      ],
      "metadata": {
        "id": "Dfvf1rdmzEKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKGczzXuoXYK"
      },
      "outputs": [],
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = HfArgumentParser(NoisifierArguments)\n",
        "    args = parser.parse_args_into_dataclasses()[0]\n",
        "    datasets = load_from_disk(args.path_to_custom_dataset)\n",
        "\n",
        "    noisifier = Noisifier(\n",
        "        datasets=datasets,\n",
        "        dataset_name=args.dataset_name,\n",
        "        level=args.noise_level,\n",
        "        type=args.augmenter_type,\n",
        "        action=args.action,\n",
        "    )\n",
        "\n",
        "    new_datasets = noisifier.augment()\n",
        "\n",
        "    # saving\n",
        "    print(f\"saving noisy dataset dict at {args.output_dir}\")\n",
        "    new_datasets.save_to_disk(args.output_dir)\n",
        "    print(new_datasets)\n",
        "\n",
        "    print(\"Loading noisy dataset dict\")\n",
        "    datasets = datasets.load_from_disk(args.output_dir)\n",
        "    print(datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CLI commands to evaluate CANINE-S on noisy data, when $p=40$\\%"
      ],
      "metadata": {
        "id": "1RULVFzGE4B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%script false --no-raise-error\n",
        "\n",
        "!python3 main.py \\\n",
        "    --model_name canine-s \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 1e-2 \\\n",
        "    --type_lr_scheduler linear \\\n",
        "    --warmup_ratio 0.1 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_steps 2500 \\\n",
        "    --num_epochs 3 \\\n",
        "    --early_stopping_patience 1 \\\n",
        "    --output_dir /content/drive/MyDrive/sentiment_analysis \\\n",
        "    --dataset_name glue \\\n",
        "    --dataset_config sst2 \\\n",
        "    --batch_size 6 \\\n",
        "    --truncation True \\\n",
        "    --padding max_length \\\n",
        "    --path_to_custom_dataset /content/drive/MyDrive/sentiment_analysis_data/noisy_data/noisy_data_40 \\\n",
        "    --mode test \\\n",
        "    --eval_only True \\\n",
        "    --device cuda \\\n",
        "    --path_to_finetuned_model /content/drive/MyDrive/sentiment_analysis/canine-s-finetuned/canine-s_best_model.pt"
      ],
      "metadata": {
        "id": "m0MtY3lpE-AL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c8c7240-e45b-4847-8bb9-c89243c1600e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Loading dataset glue\n",
            "INFO:__main__:Chosen configuration is sst2\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence', 'label', 'idx'],\n",
            "        num_rows: 63982\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence', 'label', 'idx'],\n",
            "        num_rows: 3367\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence', 'label', 'idx'],\n",
            "        num_rows: 872\n",
            "    })\n",
            "})\n",
            "INFO:__main__:Preparing for model canine-s\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Some weights of CanineForSequenceClassification were not initialized from the model checkpoint at google/canine-s and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:__main__:Tokenizing dataset\n",
            "INFO:__main__:Loading own finetuned model\n",
            "INFO:__main__:START FINAL EVALUATION\n",
            "INFO:sentiment_analysis.trainers.trainer:Start evaluation\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `CanineForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `CanineForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3367\n",
            "  Batch size = 6\n",
            "100% 562/562 [01:18<00:00,  7.15it/s]\n",
            "INFO:sentiment_analysis.trainers.trainer:Evaluation done: Eval loss 1.1804753541946411, Eval accuracy 0.7834867834867835\n",
            "INFO:__main__:Final evaluation done\n",
            "INFO:__main__:GET PREDICTIONS\n",
            "INFO:sentiment_analysis.trainers.trainer:Start predicting on test set\n",
            "The following columns in the test set  don't have a corresponding argument in `CanineForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `CanineForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 872\n",
            "  Batch size = 6\n",
            " 99% 145/146 [00:20<00:00,  7.08it/s]INFO:sentiment_analysis.trainers.trainer:Prediction done\n",
            "INFO:__main__:Predictions done\n",
            "INFO:__main__:Predictions accuracy 0.7419724770642202\n",
            "100% 146/146 [00:20<00:00,  7.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLI commands to finetune models on Sentiment140"
      ],
      "metadata": {
        "id": "jRudw65UrLL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "!python3 main.py \\\n",
        "    --model_name canine-s \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 1e-2 \\\n",
        "    --type_lr_scheduler linear \\\n",
        "    --warmup_ratio 0.1 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_steps 5000 \\\n",
        "    --num_epochs 3 \\\n",
        "    --early_stopping_patience 3 \\\n",
        "    --output_dir /content/drive/MyDrive/sentiment_analysis/sentiment_analysis_140 \\\n",
        "    --dataset_name sentiment140 \\\n",
        "    --batch_size 6 \\\n",
        "    --truncation True \\\n",
        "    --padding max_length \\\n",
        "    --path_to_custom_dataset /content/drive/MyDrive/sentiment_analysis_data/sentiment_analysis_140/sentiment140 \\\n",
        "    --mode test \\\n",
        "    --device cuda"
      ],
      "metadata": {
        "id": "NuKGBB5iFg5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLI commands to do zero-shot transfer learning on multilingual data"
      ],
      "metadata": {
        "id": "NstBt9MIXZLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%script false --no-raise-error\n",
        "\n",
        "!python3 main.py \\\n",
        "    --model_name canine-c \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 1e-2 \\\n",
        "    --type_lr_scheduler linear \\\n",
        "    --warmup_ratio 0.1 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_steps 2500 \\\n",
        "    --num_epochs 3 \\\n",
        "    --early_stopping_patience 1 \\\n",
        "    --output_dir /content/drive/MyDrive/sentiment_analysis/sentiment_analysis \\\n",
        "    --dataset_name amazon_reviews_multi \\\n",
        "    --batch_size 6 \\\n",
        "    --truncation True \\\n",
        "    --padding max_length \\\n",
        "    --path_to_custom_dataset /content/drive/MyDrive/sentiment_analysis_data/amazon_multilingual/zh \\\n",
        "    --mode test \\\n",
        "    --eval_only True \\\n",
        "    --device cuda \\\n",
        "    --path_to_finetuned_model /content/drive/MyDrive/sentiment_analysis/canine-c-finetuned/canine-c_best_model.pt"
      ],
      "metadata": {
        "id": "K0v5U2IOYc8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79451872-cba6-45f3-9d63-8c617fa0d377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:__main__:Loading dataset amazon_reviews_multi\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence', 'labels'],\n",
            "        num_rows: 160000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['sentence', 'labels'],\n",
            "        num_rows: 4000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['sentence', 'labels'],\n",
            "        num_rows: 4000\n",
            "    })\n",
            "})\n",
            "INFO:__main__:Preparing for model canine-c\n",
            "Downloading: 100% 657/657 [00:00<00:00, 660kB/s]\n",
            "Downloading: 100% 892/892 [00:00<00:00, 730kB/s]\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Downloading: 100% 698/698 [00:00<00:00, 638kB/s]\n",
            "Downloading: 100% 504M/504M [00:28<00:00, 18.8MB/s]\n",
            "Some weights of CanineForSequenceClassification were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:__main__:Tokenizing dataset\n",
            "100% 160/160 [01:48<00:00,  1.48ba/s]\n",
            "100% 4/4 [00:02<00:00,  1.50ba/s]\n",
            "100% 4/4 [00:02<00:00,  1.48ba/s]\n",
            "INFO:__main__:Loading own finetuned model\n",
            "INFO:__main__:START FINAL EVALUATION\n",
            "INFO:sentiment_analysis.trainers.trainer:Start evaluation\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `CanineForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `CanineForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4000\n",
            "  Batch size = 6\n",
            "100% 667/667 [01:33<00:00,  7.13it/s]\n",
            "INFO:sentiment_analysis.trainers.trainer:Evaluation done: Eval loss 2.561580181121826, Eval accuracy 0.56975\n",
            "INFO:__main__:Final evaluation done\n",
            "INFO:__main__:GET PREDICTIONS\n",
            "INFO:sentiment_analysis.trainers.trainer:Start predicting on test set\n",
            "The following columns in the test set  don't have a corresponding argument in `CanineForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `CanineForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Prediction *****\n",
            "  Num examples = 4000\n",
            "  Batch size = 6\n",
            "100% 666/667 [01:33<00:00,  7.13it/s]INFO:sentiment_analysis.trainers.trainer:Prediction done\n",
            "INFO:__main__:Predictions done\n",
            "INFO:__main__:Predictions accuracy 0.5485\n",
            "100% 667/667 [01:33<00:00,  7.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLI commands to do finetuning on multlingual data"
      ],
      "metadata": {
        "id": "A6RtkbHMtkHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "!python3 main.py \\\n",
        "    --model_name canine-s \\\n",
        "    --learning_rate 2e-5 \\ \n",
        "    --weight_decay 1e-2 \\  \n",
        "    --type_lr_scheduler linear \\   \n",
        "    --warmup_ratio 0.1 \\   \n",
        "    --save_strategy steps \\\n",
        "    --save_steps 2500 \\\n",
        "    --num_epochs 3 \\   \n",
        "    --early_stopping_patience 1 \\  \n",
        "    --output_dir /mnt/hdd/sentiment_analysis \\\n",
        "    --dataset_name amazon_reviews_multi \\\n",
        "    --batch_size 6 \\\n",
        "    --truncation True \\\n",
        "    --padding max_length \\\n",
        "    --path_to_custom_dataset /mnt/hdd/sentiment_analysis_data/amazon_multilingual/zh \\\n",
        "    --mode test \\\n",
        "    --device cuda "
      ],
      "metadata": {
        "id": "lRlMk0klel9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Showcase of the models capabilities "
      ],
      "metadata": {
        "id": "iFbnaVONem0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CANINE_S_FINETUNED_PATH = \"/content/drive/MyDrive/sentiment_analysis/canine-s-finetuned/canine-s_best_model.pt\"\n",
        "CANINE_C_FINETUNED_PATH = \"/content/drive/MyDrive/sentiment_analysis/canine-c-finetuned/canine-c_best_model.pt\"\n",
        "XLM_ROBERTA_FINETUNED_PATH = \"/content/drive/MyDrive/sentiment_analysis/xlm_roberta-finetuned/xlm_roberta_best_model.pt\"\n",
        "MBERT_FINETUNED_PATH = (\n",
        "    \"/content/drive/MyDrive/sentiment_analysis/mbert-finetuned/mbert_best_model.pt\"\n",
        ")\n",
        "BERT_FINETUNED_PATH = (\n",
        "    \"/content/drive/MyDrive/sentiment_analysis/bert-finetuned/bert_best_model.pt\"\n",
        ")\n",
        "DISTILBERT_FINETUNED_PATH = \"/content/drive/MyDrive/sentiment_analysis/distilbert-finetuned/distilbert_best_model.pt\"\n",
        "ROBERTA_FINETUNED_PATH = (\n",
        "    \"/content/drive/MyDrive/sentiment_analysis/roberta-finetuned/roberta_best_model.pt\"\n",
        ")\n",
        "\n",
        "CANINE_S_MODEL = \"canine-s\"\n",
        "CANINE_C_MODEL = \"canine-c\"\n",
        "BERT_MODEL = \"bert\"\n",
        "MBERT_MODEL = \"mbert\"\n",
        "XLM_ROBERTA_MODEL = \"xlm_roberta\"\n",
        "ROBERTA_MODEL = \"roberta\"\n",
        "DISTILBERT_MODEL = \"distilbert\"\n",
        "\n",
        "NUM_LABELS = 2\n",
        "\n",
        "\n",
        "def test_model(\n",
        "    model_name: str,\n",
        "    sentence: str,\n",
        "    use_finetuned_model: str,\n",
        "    true_label: int,\n",
        "    device: str,\n",
        ") -> None:\n",
        "    model, tokenizer = _loading_model_and_tokenizer(model_name)\n",
        "    path_to_finetuned_model = _get_finetuned_model_path(model_name)\n",
        "    print()\n",
        "    print(\"Chosen sentence: \", sentence)\n",
        "\n",
        "    if use_finetuned_model == \"Yes\":\n",
        "        use_finetuned_model = True\n",
        "    else:\n",
        "        use_finetuned_model = False\n",
        "\n",
        "    if use_finetuned_model:\n",
        "        print()\n",
        "        print(\"Using own finetuned model on SST2\")\n",
        "        print(\"Loading finetuned model\")\n",
        "        model.load_state_dict(torch.load(path_to_finetuned_model, map_location=device))\n",
        "        _helper_test(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            sentence,\n",
        "            true_label,\n",
        "        )\n",
        "    else:\n",
        "        print(\"Using initial model, not finetuned for prediction:\")\n",
        "        _helper_test(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            sentence,\n",
        "            true_label,\n",
        "        )\n",
        "\n",
        "\n",
        "def _helper_test(\n",
        "    model: Model,\n",
        "    tokenizer: PreTrainedTokenizer,\n",
        "    sentence: str,\n",
        "    label: int,\n",
        ") -> None:\n",
        "    inputs = tokenizer(\n",
        "        sentence,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    labels = torch.tensor(label)\n",
        "    outputs = model(**inputs, labels=labels)\n",
        "    predicted_class_id = outputs.logits.argmax().item()\n",
        "\n",
        "    print(f\"Model predicted class: {predicted_class_id}\")\n",
        "    print(f\"Loss is: {round(outputs.loss.item(), 2)}\")\n",
        "\n",
        "\n",
        "def _loading_model_and_tokenizer(model_name: str) -> Tuple[Model, PreTrainedTokenizer]:\n",
        "    if model_name in [CANINE_C_MODEL, CANINE_S_MODEL]:\n",
        "        pretrained_model_name = f\"google/{model_name}\"\n",
        "        tokenizer = CanineTokenizer.from_pretrained(pretrained_model_name)\n",
        "        model = CanineForSequenceClassification.from_pretrained(\n",
        "            pretrained_model_name, num_labels=NUM_LABELS\n",
        "        )\n",
        "    else:\n",
        "        if model_name == BERT_MODEL:\n",
        "            pretrained_model_name = \"bert-base-uncased\"\n",
        "            tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name)\n",
        "            model = BertForSequenceClassification.from_pretrained(\n",
        "                pretrained_model_name, num_labels=NUM_LABELS\n",
        "            )\n",
        "\n",
        "        elif model_name == MBERT_MODEL:\n",
        "            pretrained_model_name = \"bert-base-multilingual-cased\"\n",
        "            tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name)\n",
        "            model = BertForSequenceClassification.from_pretrained(\n",
        "                pretrained_model_name, num_labels=NUM_LABELS\n",
        "            )\n",
        "\n",
        "        elif model_name == XLM_ROBERTA_MODEL:\n",
        "            pretrained_model_name = \"xlm-roberta-base\"\n",
        "            tokenizer = XLMRobertaTokenizerFast.from_pretrained(pretrained_model_name)\n",
        "            model = RobertaForSequenceClassification.from_pretrained(\n",
        "                pretrained_model_name, num_labels=NUM_LABELS\n",
        "            )\n",
        "\n",
        "        elif model_name == ROBERTA_MODEL:\n",
        "            pretrained_model_name = \"roberta-base\"\n",
        "            tokenizer = RobertaTokenizerFast.from_pretrained(pretrained_model_name)\n",
        "            model = RobertaForSequenceClassification.from_pretrained(\n",
        "                pretrained_model_name, num_labels=NUM_LABELS\n",
        "            )\n",
        "\n",
        "        elif model_name == DISTILBERT_MODEL:\n",
        "            pretrained_model_name = \"distilbert-base-uncased\"\n",
        "            tokenizer = DistilBertTokenizerFast.from_pretrained(pretrained_model_name)\n",
        "            model = DistilBertForSequenceClassification.from_pretrained(\n",
        "                pretrained_model_name, num_labels=NUM_LABELS\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def _get_finetuned_model_path(model_name: str) -> str:\n",
        "    if model_name == CANINE_C_MODEL:\n",
        "        path_to_finetuned_model = CANINE_C_FINETUNED_PATH\n",
        "    elif model_name == CANINE_S_MODEL:\n",
        "        path_to_finetuned_model = CANINE_S_FINETUNED_PATH\n",
        "    elif model_name == XLM_ROBERTA_MODEL:\n",
        "        path_to_finetuned_model = XLM_ROBERTA_FINETUNED_PATH\n",
        "    elif model_name == MBERT_MODEL:\n",
        "        path_to_finetuned_model = MBERT_FINETUNED_PATH\n",
        "    elif model_name == BERT_MODEL:\n",
        "        path_to_finetuned_model = BERT_FINETUNED_PATH\n",
        "    elif model_name == DISTILBERT_MODEL:\n",
        "        path_to_finetuned_model = DISTILBERT_FINETUNED_PATH\n",
        "    elif model_name == ROBERTA_MODEL:\n",
        "        path_to_finetuned_model = ROBERTA_FINETUNED_PATH\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    return path_to_finetuned_model\n"
      ],
      "metadata": {
        "id": "7xNf1oY7esjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_model_name = input(\"Choose a model; names are in the list above:\")\n",
        "user_use_finetuned_model = input(\"Do you want to use our finetuned model in STT2 ? Yes/No \")\n",
        "user_sentence = input(\"Type a sentence: \")\n",
        "user_label = input(\n",
        "    \"Is this sentence negative (0) or positive (1) - put corresponding int: \"\n",
        ")\n",
        "while user_model_name != \"quit\":\n",
        "    test_model(\n",
        "        model_name=user_model_name,\n",
        "        sentence=user_sentence,\n",
        "        use_finetuned_model=user_use_finetuned_model,\n",
        "        true_label=int(user_label),\n",
        "        device=\"cpu\",\n",
        "    )\n",
        "    print()\n",
        "    user_model_name = input(\"Choose a model; names are in the list above:\")\n",
        "    user_use_finetuned_model = input(\n",
        "        \"Do you want to use our finetuned model in STT2 ? Yes/No \"\n",
        "    )\n",
        "    user_sentence = input(\"Type a sentence: \")\n",
        "    user_label = input(\n",
        "        \"Is this sentence negative (0) or positive (1) - put corresponding int: \"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6503JP1mhFmJ",
        "outputId": "114a57a6-9b94-476c-f0b7-f349be9b9dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Choose a model; names are in the list above:canine-s\n",
            "Do you want to use our finetuned model in STT2 ? Yes/NoYes\n",
            "Type a sentence: This is cleary bad ! What are you doing ?? Stop it !\n",
            "Is this sentence negative (0) or positive (1) - put corresponding int: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Chosen sentence:  This is cleary bad ! What are you doing ?? Stop it !\n",
            "\n",
            "Using own finetuned model on SST2\n",
            "Loading finetuned model\n",
            "Model predicted class: 0\n",
            "Loss is: 0.0\n",
            "\n",
            "Choose a model; names are in the list above:canine-s\n",
            "Do you want to use our finetuned model in STT2 ? Yes/No Yes\n",
            "Type a sentence: Yes I think your right, this is great ! \n",
            "Is this sentence negative (0) or positive (1) - put corresponding int: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Chosen sentence:  Yes I think your right, this is great ! \n",
            "\n",
            "Using own finetuned model on SST2\n",
            "Loading finetuned model\n",
            "Model predicted class: 1\n",
            "Loss is: 0.0\n",
            "\n",
            "Choose a model; names are in the list above:canine-s\n",
            "Do you want to use our finetuned model in STT2 ? Yes/No No\n",
            "Type a sentence: Arrête, tu racontes n'importe quoi, ça m'énerve !\n",
            "Is this sentence negative (0) or positive (1) - put corresponding int: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Chosen sentence:  Arrête, tu racontes n'importe quoi, ça m'énerve !\n",
            "Using initial model, not finetuned for prediction:\n",
            "Model predicted class: 0\n",
            "Loss is: 0.55\n",
            "\n",
            "Choose a model; names are in the list above:canine-s\n",
            "Do you want to use our finetuned model in STT2 ? Yes/No Yes\n",
            "Type a sentence: Arrête, tu racontes n'importe quoi, ça m'énerve !\n",
            "Is this sentence negative (0) or positive (1) - put corresponding int: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Chosen sentence:  Arrête, tu racontes n'importe quoi, ça m'énerve !\n",
            "\n",
            "Using own finetuned model on SST2\n",
            "Loading finetuned model\n",
            "Model predicted class: 0\n",
            "Loss is: 0.0\n",
            "\n",
            "Choose a model; names are in the list above:quit\n",
            "Do you want to use our finetuned model in STT2 ? Yes/No quit\n",
            "Type a sentence: quit\n",
            "Is this sentence negative (0) or positive (1) - put corresponding int: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "yqtXbDeGYvCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binary Sentiment Classification on SST2\n",
        "\n",
        "Models were trained with the following parameters:\n",
        "\n",
        "|             \t| Batch size \t| Learning Rate \t| Weigh decay \t| Nb of epochs \t | Number of training examples \t| Number of validation examples \t| Lr scheduler \t| Warmup ratio \t|\n",
        "|-------------\t|------------\t|---------------\t|-------------\t|----------------|-----------------------------\t|-------------------------------\t|--------------\t|--------------\t|\n",
        "| RoBERTa     \t| 12         \t| 2e-5          \t| 1e-2        \t| 3            \t | 63981                       \t| 872                           \t| linear       \t| 0.1          \t|\n",
        "| BERT        \t| 12         \t| 2e-5          \t| 1e-2        \t| 3            \t | 63981                       \t| 872                           \t| linear       \t| 0.1          \t|\n",
        "| DistilBERT  \t| 12         \t| 2e-5          \t| 1e-2        \t| 3            \t | 63981                       \t| 872                           \t| linear       \t| 0.1          \t|\n",
        "| mBERT       \t| 12         \t| 2e-5          \t| 1e-2        \t| 3            \t | 63981                       \t| 872                           \t| linear       \t| 0.1          \t|\n",
        "| XLM-ROBERTA \t| 12         \t| 2e-5          \t| 1e-2        \t| 3            \t | 63981                       \t| 872                           \t| linear       \t| 0.1          \t|\n",
        "| CANINE-c    \t| 6          \t| 2e-5          \t| 1e-2        \t| 3            \t | 63981                       \t| 872                           \t| linear       \t| 0.1          \t|\n",
        "| CANINE-s    \t| 6          \t| 2e-5          \t| 1e-2        \t| 3            \t | 63981                       \t| 872                           \t| linear       \t| 0.1          \t|\n",
        "\n",
        "\n",
        "Obtained results:\n",
        "\n",
        "|   Accuracy  \t| Val set \t| Test set \t|\n",
        "|:-----------:\t|:-------:\t|:--------:\t|\n",
        "|     BERT    \t|   0.94  \t|   0.93   \t|\n",
        "|   RoBERTa   \t|   0.94  \t|   0.94   \t|\n",
        "|  DistilBERT \t|   0.94  \t|   0.91   \t|\n",
        "|    mBERT    \t|   0.93  \t|   0.88   \t|\n",
        "| XLM-RoBERTa \t|   0.92  \t|   0.92   \t|\n",
        "|   CANINE-C  \t|   0.93  \t|   0.86   \t|\n",
        "|   CANINE-S  \t|   0.92  \t|   0.85   \t|\n",
        "\n",
        "In this setting, both CANINE-S and CANINE-C perform decently well on the validation set but not as much as the test set.\n",
        "There are 8 percentage points of difference between CANINE-C and RoBERTa for instance. mBERT has similar behavior than\n",
        "the two CANINE models.\n"
      ],
      "metadata": {
        "id": "E7zIYVUwGKG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Robustness to noise\n",
        "\n",
        "In this experience, the goal is to evaluate the models' robustness of noise. To do so, we created 3 noisy versions of the\n",
        "SST2 dataset where the sentences have been artificially enhanced with noisy (in our case we chose ``RandomCharAug``\n",
        "from ``nlpaug`` library with action `substitute` but in our package 4 other types of noise have been developed - refer \n",
        "to `noisifier/noisifier.py`).\n",
        "\n",
        "Three levels of noise were chosen: 10\\%, 20\\% and 40\\% . Each word gets transformed with probability $p$ into a misspelled \n",
        "version of it (see [nlpaug documentation](https://github.com/makcedward/nlpaug/blob/master/nlpaug/augmenter/char/random.py)\n",
        "for more information).\n",
        "\n",
        "The noise is **only** applied to the SST2 validation and test sets made of 3368 and 872 examples respectively. \n",
        "We compared the 7 models we finetuned on the clean version of SST2 (first experiment) on these 3 noisy datasets (on for \n",
        "each level of $p$). The following table gathers the results (averaged over 3 runs):\n",
        "\n",
        "|             \t| Noise level 10% \t|          \t| Noise level 20% \t|          \t| Noise level 40% \t|          \t|\n",
        "|:-----------:\t|:---------------:\t|:--------:\t|:---------------:\t|:--------:\t|:---------------:\t|:--------:\t|\n",
        "|             \t|     Val set     \t| Test set \t|     Val set     \t| Test set \t|     Val set     \t| Test set \t|\n",
        "|     BERT    \t|       0.88      \t|   0.87   \t|       0.85      \t|   0.82   \t|       0.80      \t|   0.80   \t|\n",
        "|   RoBERTa   \t|       0.88      \t|   0.89   \t|       0.87      \t|   0.85   \t|       0.83      \t|   0.82   \t|\n",
        "|  DistilBERT \t|       0.85      \t|   0.82   \t|       0.82      \t|   0.79   \t|       0.76      \t|   0.76   \t|\n",
        "|    mBERT    \t|       0.88      \t|   0.82   \t|       0.85      \t|   0.80   \t|       0.80      \t|   0.76   \t|\n",
        "| XLM-RoBERTa \t|       0.89      \t|   0.85   \t|       0.86      \t|   0.83   \t|       0.81      \t|   0.81   \t|\n",
        "|   CANINE-C  \t|       0.86      \t|   0.80   \t|       0.83      \t|   0.76   \t|       0.79      \t|   0.74   \t|\n",
        "|   CANINE-S  \t|       0.85      \t|   0.80   \t|       0.83      \t|   0.77   \t|       0.78      \t|   0.74   \t|\n",
        "\n",
        "Both CANINE models have a better performance than DistilBERT for a high level of noise (>= 40\\%). However all other models\n",
        "are better to handle this type of artificial noise, RoBERTa being the best of all. "
      ],
      "metadata": {
        "id": "i2Gjj3MRHTGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Classification on more challenging Sentiment140 dataset (tweets)\n",
        "\n",
        "The following experience is meant to evaluate the performances of the various models on a more challenging dataset: \n",
        "Sentiment140. This dataset is made of 1.6 million of tweets, all in English. The language used is very different from the\n",
        "one in SST2 as it is made of more abbreviations, colloquialisms, slang, etc. Therefore it is expected to be hard for the\n",
        "models to handle such text (which is \"naturally\" noisy). CANINE has a theoretical advantage on such dataset due to the\n",
        "fact that it is tokenizer-free and operates at the character level.\n",
        "\n",
        "The following table reports the results we obtained when finetuning all models on the (smaller) training set of 63360 examples.\n",
        "\n",
        "|                 \t| **Val set** \t | **Test set** \t |\n",
        "|:---------------:\t|:-------------:|:--------------:|\n",
        "|     **BERT**    \t|   0.84    \t   |   0.86     \t   |\n",
        "|   **RoBERTa**   \t|   0.87    \t   |   0.86     \t   |\n",
        "|  **DistilBERT** \t|   0.83    \t   |   0.85     \t   |\n",
        "|    **mBERT**    \t|   0.79    \t   |   0.78     \t   |\n",
        "| **XLM-RoBERTa** \t|   0.81    \t   |   0.80     \t   |\n",
        "|   **CANINE-C**  \t|   0.79    \t   |   0.78     \t   |\n",
        "|   **CANINE-S**  \t|   0.80    \t   |   0.79     \t   |\n"
      ],
      "metadata": {
        "id": "KeioaiqFHhIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Few-shot learning and domain adaptation\n",
        "\n",
        "The goal of this experiment is to measure the ability of CANINE (and other models) to transfer to unseen data, in \n",
        "another domain. This could either be done in zero-shot or few-shot settings. Here we decided to go with the latter as it \n",
        "is more realistic. In real life, a company might already have a custom small database of labeled documents and questions \n",
        "associated (manually created) but would want to deploy a Question Answering system on the whole unlabeled database. The \n",
        "CUAD dataset is perfect for this task as it is highly specialized (legal domain, legal contract review). The training set \n",
        "is made of 22450 question/context pairs and the test set of 4182. We randomly selected 1\\% of the training set (224 examples) \n",
        "to train on for 3 epochs, using the previously finetuned models on SQuADv2. Then each model was evaluated on 656 test examples. \n",
        "Results are reported in the following table and to ensure fair comparison, all models where trained and tested on the \n",
        "exact same examples. \n",
        "\n",
        "|                 \t| **F1 score** \t| **EM score** \t|\n",
        "|:---------------:\t|:------------:\t|:------------:\t|\n",
        "|     **BERT**    \t|     74.18    \t|     72.72    \t|\n",
        "|   **RoBERTa**   \t|     73.83    \t|     72.24    \t|\n",
        "|  **DistilBERT** \t|     72.86    \t|     71.37    \t|\n",
        "|    **mBERT**    \t|     74.50    \t|     73.12    \t|\n",
        "| **XLM-RoBERTa** \t|     76.64    \t|     73.44    \t|\n",
        "|   **CANINE-C**  \t|     72.51    \t|     71.39    \t|\n",
        "|   **CANINE-S**  \t|     72.27    \t|     71.27    \t|"
      ],
      "metadata": {
        "id": "LtBDkzMsHi2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-shot transfer learning and domain adaptation from SST2 to Sentiment140\n",
        "\n",
        "In this experience we would like to see how CANINE models perform when they are faced with \"natural\" noise that they were\n",
        "**not** trained on. Compared to the previous experience where models where trained on Sentiment140, here models are \n",
        "trained on SST2 but evaluated on validation and test set from Sentiment140. \n",
        "\n",
        "In the previous task, CANINE models were not the best performing one. Actually, with mBERT, they were the last ones. Here \n",
        "we are evaluating something different: the ability for a model to adapt to another domain (in the sense that the topic \n",
        "and the way of writing/speaking are different) in a zero-shot transfer setting. It might be that, in real life settings, \n",
        "one has access to a clean benchmark-type dataset (such as SST2) but wants to do inference on a dataset whose subject is\n",
        "quite different and full of misspellings and grammar errors. \n",
        "\n",
        "Results are reported in the following table:\n",
        "\n",
        "|                 \t| **Val set** \t| **Test set** \t|\n",
        "|:---------------:\t|:-----------:\t|:------------:\t|\n",
        "|     **BERT**    \t|     0.72    \t|     0.84     \t|\n",
        "|   **RoBERTa**   \t|     0.73    \t|     0.88     \t|\n",
        "|  **DistilBERT** \t|     0.71    \t|     0.82     \t|\n",
        "|    **mBERT**    \t|     0.68    \t|     0.76     \t|\n",
        "| **XLM-RoBERTa** \t|     0.72    \t|     0.83     \t|\n",
        "|   **CANINE-C**  \t|     0.64    \t|     0.77     \t|\n",
        "|   **CANINE-S**  \t|     0.64    \t|     0.73     \t|\n",
        "\n",
        "CANINE models do not perform well on this task. They have -9 percentage point of accuracy compared to RoBERTa for \n",
        "instance (best performing model on this task) on the validation set. We noticed that mBERT has more difficulties than \n",
        "other BERT-like models on Sentiment140 dataset overall. Again, CANINE and mBERT have similar behavior."
      ],
      "metadata": {
        "id": "0lQjksyvHlS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-shot transfer learning on multlingual data\n",
        "\n",
        "This experiment builds on the idea that CANINE is expected to perform better on languages with a different morphology\n",
        "than English, for instance on non-concatenative morphology (such as Arabic and Hebrew), compounding (such as German and\n",
        "Japanese), vowel harmony (Finnish), etc. Moreover, it is known that splitting on whitespaces (which is often done in most\n",
        "tokenizer - note that SentencePiece has an option to skip whitespace splitting) is not adapted to languages such as Thai \n",
        "or Chinese. \n",
        "\n",
        "In this experience, models have been finetuned on the English dataset SST2 and are only evaluated both on validation and\n",
        "tests sets of 4 languages from the Multilingual Amazon Reviews Corpus ([MARC](https://arxiv.org/abs/2010.02573)). We \n",
        "considered the four following language: German, French, Japanese and Chinese for their morphological properties. \n",
        "\n",
        "This dataset contains for each review the number of stars associated by the reviewer. To derive positive/negative\n",
        "sentiment from this, we considered that if 1 or 2 stars only have been associated to the review, the sentiment is \n",
        "negative. While if 4 or 5 stars have been chosen, the review is positive. Neutral reviews, with 3 stars, were not \n",
        "considered. For each language, this gives us 160000 training samples, 4000 validation samples and 4000 test samples.\n",
        "\n",
        "Results are given in the following table:\n",
        "\n",
        "|                 \t|  **French** \t|              \t|  **German** \t|              \t| **Japanese** \t|              \t| **Chinese** \t|              \t|\n",
        "|:---------------:\t|:-----------:\t|:------------:\t|:-----------:\t|:------------:\t|:------------:\t|:------------:\t|:-----------:\t|:------------:\t|\n",
        "|                 \t| **Val set** \t| **Test set** \t| **Val set** \t| **Test set** \t|  **Val set** \t| **Test set** \t| **Val set** \t| **Test set** \t|\n",
        "|    **mBERT**    \t|     0.71    \t|     0.70     \t|     0.66    \t|     0.66     \t|     0.56     \t|     0.55     \t|     0.58    \t|     0.59     \t|\n",
        "| **XLM-RoBERTa** \t|     0.87    \t|     0.86     \t|     0.86    \t|     0.87     \t|     0.87     \t|     0.85     \t|     0.80    \t|     0.79     \t|\n",
        "|   **CANINE-C**  \t|     0.70    \t|     0.69     \t|     0.59    \t|     0.58     \t|     0.50     \t|     0.50     \t|     0.57    \t|     0.55     \t|\n",
        "|   **CANINE-S**  \t|     0.71    \t|     0.70     \t|     0.61    \t|     0.61     \t|     0.52     \t|     0.52     \t|     0.57    \t|     0.57     \t|\n",
        "\n",
        "CANINE-S is similar to mBERT for French and Chinese data. Overall XLM-RoBERTa is extremely better than other models. \n",
        "Note that its pre-training strategy is different from the one of mBERT and CANINE. Indeed, while mBERT and CANINE have both been \n",
        "pretrained on the top 104 languages with the largest Wikipedia using a MLM objective, XLM-RoBERTa was pretrained on 2.5TB \n",
        "of filtered CommonCrawl data containing 100 languages. This might be a confounding variable."
      ],
      "metadata": {
        "id": "5CNijOvhdTph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetuning on multlingual data\n",
        " \n",
        "In this last experiment, we now compare CANINE to other BERT-like models on multilingual data where they are finetuned \n",
        "on it. This is the difference with the previous experience. To do so, we have chosen to work again with the MARC dataset, \n",
        "using data in German, Japanese and Chinese. We would like to see how CANINE compares and if it is better on languages\n",
        "which are more challenging for token-based models (Chinese for instance). \n",
        "\n",
        "Please note that due to time and compute constraints, we considered only one CANINE model, CANINE-S. \n",
        "\n",
        "The results are given below:\n",
        "\n",
        "|             \t|  German \t|          \t| Japanese \t|          \t| Chinese \t|          \t|\n",
        "|:-----------:\t|:-------:\t|:--------:\t|:--------:\t|:--------:\t|:-------:\t|:--------:\t|\n",
        "|             \t| Val set \t| Test set \t|  Val set \t| Test set \t| Val set \t| Test set \t|\n",
        "|    mBERT    \t|   0.93  \t|   0.93   \t|   0.92   \t|   0.92   \t|   0.87  \t|   0.88   \t|\n",
        "| XLM-RoBERTa \t|   0.92  \t|   0.92   \t|   0.93   \t|   0.93   \t|   0.88  \t|   0.88   \t|\n",
        "|   CANINE-S  \t|   0.93  \t|   0.93   \t|   0.90   \t|   0.89   \t|   0.85  \t|   0.85   \t|\n",
        "\n",
        "Quite surprisingly, on German, CANINE-S is slightly better than XLM-RoBERTa and has similar performance than mBERT. \n",
        "However on Japanese and Chinese, it is not the case. mBERT and especially XLM-RoBERTa should be preferred has they\n",
        "provide better accuracy on both validation and test sets. "
      ],
      "metadata": {
        "id": "PilaHh45dTg7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of prediction errors on SST2 dataset\n",
        "\n",
        "\n",
        "For this section, please take a look at the following [Colab notebook]()."
      ],
      "metadata": {
        "id": "jLRZKavvddWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion\n",
        "\n",
        "From our SC experiments, overall, other BERT-like models were better than CANINE. However note that on most tasks, CANINE performs similarly to mBERT and might be even slightly better. But for general tasks in English or more generally with languages for which we have a lot of resources, XLM-R and/or RoBERTa are better. We were not able to prove in our experiments that CANINE is better than tokenizer-based BERT-like models even on more challenging and complex languages such as Thai, Chinese, Japanese or Arabic. When finetuning for binary classification on German, Japanese and Chinese, CANINE-S was slightly better than XLM-R on German (high proximity with English, West Germanic family). But this was not the case in Japanese and Chinese where mBERT and XLM-R should be prefered (+3pp in accuracy). "
      ],
      "metadata": {
        "id": "xEBA5-GXdgWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "q3sxmokZdhXA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ML_NLP_sentiment_analysis_CANINE.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}